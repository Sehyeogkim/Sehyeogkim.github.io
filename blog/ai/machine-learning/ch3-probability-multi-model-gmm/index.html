<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ch3 Probability - Multi model - GMM | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../../assets/css/style.css">
  <script>!function(){var t=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",t)}();</script>
</head>
<body>
  <button class="theme-toggle" aria-label="Toggle theme"><svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg><svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg></button>

  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../../assets/images/bg.jpg" alt="Background" onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo" src="../../../../assets/images/profile.jpg" alt="Sehyeog Kim"
             onerror="this.style.background='#eaeef2'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Personal Blog</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="15" height="15" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-group-label">AI</span>
        <a href="/blog/ai/agentic-ai-theory/" class="nav-item">Agentic_AI_Theory<span class="nav-post-count">8</span></a>
        <a href="/blog/ai/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/ai/machine-learning/" class="nav-item active">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/ai/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <span class="nav-group-label">BioMechanics</span>
        <a href="/blog/biomechanics/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/biomechanics/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <span class="nav-group-label">Mechanical_Engineering</span>
        <a href="/blog/mechanical-engineering/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/mechanical-engineering/computational-fluid-dynamics/" class="nav-item">Computational_Fluid_Dynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/mechanical-engineering/engineering-mathematics/" class="nav-item">Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/mechanical-engineering/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/mechanical-engineering/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/mechanical-engineering/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/mechanical-engineering/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/mechanical-engineering/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <main class="main-content">
      <div class="breadcrumb"><a href="/">Home</a><span class="sep">/</span><a href="/blog/ai/">AI</a><span class="sep">/</span><a href="/blog/ai/machine-learning/">Machine_Learning</a><span class="sep">/</span><span>Ch3 Probability - Multi model - GMM</span></div>
<a href="/blog/ai/machine-learning/" class="back-link">&larr; Back to Machine_Learning</a>
<div class="page-header"><h1>Ch3 Probability - Multi model - GMM</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-12</span><span class="meta-item"><span class="meta-label">Category:</span> Machine_Learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/m/234" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>Ch3 Probability - Multi model - GMM<br />
우리의 목표는 주어진 데이터를 가지고,<br />
데이터의 분포를 설명하는 하나의 모델을 만드는 것.<br />
(그래야 주어지지 않은 데이터의 값을 예측할 수 있다.)<br />
그 모델을 우리는 지금까지 어떤 하나의 모델로 해석해야 할까?<br />
라는 질문을 가지고 접근했다.<br />
However,<br />
실제 데이터들은 사실상 분포 하나로는 모델링이 불가능한 경우가 대부분이다.<br />
그렇다면, 데이터를 여러 sub model의 집합이라고 생각해보면?<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-001.png" /><br />
즉, subgroup들은 특정분포를 따르고,<br />
그 subgroup들의 합으로 전체 data set을 구성한다면???<br />
위 아이디어를 수학적으로 표현하면 다음과 같다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-002.png" /><br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-003.png" /><br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-002.png" /><br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-003.png" /><br />
여기서<br />
pi(가중치)<br />
또한 우리는 확률로 계산을 할 것이고,<br />
이를 위해 Latent Variable z 를 정의한다. (뒤에서 자세하게 설명)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-004.png" /><br />
즉, parameter<br />
θ에 대해서 z = k 일 확률을 pi_k 로 정의한다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-005.png" /><br />
0&lt;가중치 = pi_k &lt;1이므로 확률 = 가중치로 해석하기 적절.<br />
그리고 여기서 conditional Likelihood는 다음과 같이 정의 가능하다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-006.png" /><br />
예를들어, 총 3개의 subgroup 확률분포가 있고,<br />
각각의 가중치를 pi1,pi2,pi3이라고 하자.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-007.png" /><br />
subgroup과 가중치를 연결하는 index -&gt; latent variable<br />
여기서 한발짝 나아가서 가중치는 z에 대한 확률이었다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-008.png" /><br />
다시 표현하면 다음과 같다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-009.png" /><br />
위 Multimodel중에서 가장 많이 사용하는 모델이<br />
subgroup의 확률분포가 Gaussian Distribution이라는 가정을 한,<br />
Gaussian Mixture model<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-010.png" /><br />
(가우시안 분포는 가장 쉽고 모델링하기 좋다, 평균과 분산만 있으면 모델링이 끝나기 때문)<br />
아래 그림을 보면, 3개의 Subgroup들 각각 Gaussisan distribution임을 알 수 있고,<br />
(b)의 전체 확률은 3개의 확률을 적절히 더하여 구성된다.(적절히 means multiplied w/ weight function)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-011.png" /><br />
여기서, (주어진 데이터를 가지고) 우리가 구해야하는 것은<br />
가중치와, 각 Gaussian 분포의 평균과 분산<br />
을 구해야한다.<br />
어떻게 구하는지 과정을 살펴보자.<br />
n개의 데이터가 있다고 가정하고,<br />
각 데이터의 Dimension은 (D x 1) vector라고 하자.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-012.png" /><br />
e.g.) D개의 센서로 n번 각각 관찰.<br />
총 데이터셋은 D 집합으로 다음과 같이 표현한다.(위의 DIMENSION D와 다르다!!)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-013.png" /><br />
(쉽게 표현하면 아래와 같은 전체데이터셋 집합이라고 생각하면 된다)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-014.png" /><br />
다시 말하지만, y 각각은 (D개의 정보가 담겨있음)<br />
Goal: Find the θ corresponds to the max Likelihood<br />
우리의 최종목표는 내가 가지고있는 데이터 D와 가장 잘맞는<br />
θ<br />
를 찾는 것)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-015.png" /><br />
그리고<br />
GMM에서 Theata는 가중치와 각 가우시안 분포의 평균과 분산임을<br />
다시. 강조!.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-016.png" /><br />
최적의 theat를 찾는 알고리즘은 여러가지가 있지만,<br />
"EM 알고리즘이 가장 널리 알려진 방법이지만, 실제로는 확률적 경사하강법(SGD)이나 변분추론(VI), MCMC 샘플링, 모멘트 방법 등 다양한 대안 알고리즘도 존재한다. 다만 구현 난이도와 계산 효율성 때문에 실무에서는 대부분 EM이나<br />
그 변형(Online EM, Stochastic EM)을 사용한다"<br />
(다른 방법들은 추후 블로그에서 다룰예정)<br />
대표적인 EM 알고리즘을 살펴보자.<br />
0.<br />
θ ( π k ​ , μ k ​ , Σ k ​ ) initalize<br />
1.<br />
θ -&gt; responsibility on each latent variable 계산.<br />
2. responsibility 를 weight로 사용하여, θ( π k ​ , μ k ​ , Σ k ​) update<br />
3. 데이터에 대해서 가장 likelihood 가 높은 θ가 나오면 stop.<br />
(otherwise go to step 1)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-017.png" /><br />
여기서 1,2,3 과정을 지금부터 자세하게 살펴보자.<br />
1. Find the Latent variable (z) associated to the data yn<br />
GMM의 가장 중요하고 핵심인<br />
Latent Variable<br />
즉 우리는 yn -&gt; output 사이에 latent varibale zn을 넣는다.<br />
(그 이유를 생각하면서 살펴보자)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-018.png" /><br />
zn is a called as "label" from 1 to K<br />
우리는 각 데이터와<br />
θ<br />
에 대해서, zn = 1,2 ....K 일때의 Conditional 확률을 각각 구한다.<br />
그리고 그 조건부확률을<br />
Responsibility<br />
라고 정의한다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-019.png" /><br />
data : yn에 대해, zn = k에 대해 -&gt; reponsiblity = rnk<br />
(일단 왜 이런 꼴인지는 넘어가자, 블로그 마지막 부록에서 다루도록 하겠습니다..이해해주세요)<br />
어렵다. 이해하기 굉장히 난해하다.<br />
그럴때는 예시로 이해해보자.<br />
[Example]<br />
학생 30명이 반에 있고<br />
우리는 학생의 수학점수를 데이터 yn이라고 하자.<br />
(e.g.) y1: 학생1의 수학점수<br />
여기서 우리는 학생 10명의 수학성적만 알고 있다고 가정하자.<br />
그랬을때, 나머지 20명의 점수 도 알고 싶다.<br />
(즉, 수학성적이 어떻게 분포하는 지를 알고 싶다)<br />
굉장히 난해하고, 복잡하고 하나의 Gaussian distribution으로는 해석이 불가능하므로<br />
우리는<br />
(K = 3)명의 수학선생 님<br />
이 존재한다고 가정한다.<br />
여기서 Latent Variable = {선생님1, 선생님2, 선생님3}<br />
Label(k) = 선생님 번호<br />
(즉, 3개의 Gaussian Distribution의 적절한 합으로 학생의 점수가 분포한다고 가정하는 것이다!!!!!!!!!)<br />
따라서<br />
,<br />
yn이 주어졌을때<br />
각 수학선생님의 contribution을 우리는 구하여,<br />
최종 yn을 예측하는 모델을 만들 계획.<br />
여기서,<br />
하나의 학생(데이터)에 대해서<br />
latent variable(선생님)의 기여도가 responsibility라는 것이다.<br />
가지고 있는 데이터를 활용하여 (학생1의 데이터는 암)<br />
학생1에 대해서 선생님1,2,3의 기여도를 구한결과.<br />
(0.1, 0.8, 0.1)<br />
즉,<br />
r11 = p(z1 = 1|y1, θ ) = 0.1<br />
r12 = p(z1 = 2|y1, θ ) = 0.8<br />
r13 = p(z1 = 3|y1, θ ) = 0.8<br />
(다시 그림과 연결해보면)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-011.png" /><br />
ㅍ<br />
위 사진처럼 초록, 빨강, 파랑 선생님이 있고,<br />
3명의 선생님 = 3개의 작은 Gaussian distribution이고,<br />
각각 z1,z2,z3으로 라벨링을 하여,<br />
1번 Gaussian Distribution의 기여도 = 0.1<br />
2번 Gaussian Distribution의 기여도 = 0.8<br />
3번 Gaussian Distribution의 기여도 = 0.1<br />
인 것<br />
2. update the θ ( π k ​ , μ k ​ , Σ k ​)<br />
우리는 위과정으로,<br />
가지고 있는 10명의 학생마다 기여도를 모두. 구했다고 가정하자.<br />
학생1 = (0.1, 0.8, 0.1)<br />
학생2 =. (0.2, 0.7, 0.1)<br />
...<br />
학생10 =. (r10,1 r10,2 , r10,3)<br />
여기서 각 선생님마다 전체 학생의 기여도를 각각 구해준다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-020.png" /><br />
N1 = 선생님1의 전체. 기여도 =.( 0.1 +. 0.2 +. .. r10,1)<br />
N2 = 선생님2의 전체. 기여도 =.( 0.8 +. 0.7 +. .. r10,2)<br />
N3 = 선생님3의 전체. 기여도 =.( 0.1 +. 0.1 +. .. r10,3)<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-021.png" /><br />
N = N1 + N2 + N3<br />
즉, 평균적으로 선생님의 기여도가 얼마인지를 구하고,<br />
이는 그 선생님의 평균기여도 pi로 새롭게 업데이트해준다.<br />
3개의 가우시안 분포의 적절한 조합으로 하나의 가우시안 분포를 만드는게 우리의 목표이고,<br />
그 적절한 조합이랑 가중치를 적절히 곱해주는 것.<br />
그 가중치는 가지고 있는 데이터에서 나온 평균 기여도(<br />
responsibility<br />
)로 구한다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-022.png" /><br />
이제 각. 가우시안 분포의 평균은 다음의 식으로 업데이트 해준다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-023.png" /><br />
즉 각 학생마다 기여도와 점수를 곱해준 값을 전부 더하고,<br />
그 가우시안 분포의 총 기여도로 나누어 준다.<br />
이제 마지막으로 각 가우시안 분포마다 분산을 다음과 같이 계산하여 업데이트 해주자.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-024.png" /><br />
즉 각 학생(데이터)가 평균으로부터 떨어진 정도 (분산)을 구하고 기여도를 곱해준 값을 모두 더하고,<br />
전체 기여도로 나누어 주는 것.<br />
(굉장히 논리적이고 합리적이다)<br />
3. check MLE θ<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-017.png" /><br />
이제 업데이트한 가중치가 가장 likelihood가 데이터에 대해서 높은지를 확인한다.<br />
(추후에 자세하게 설명 예정)<br />
즉, 지금의<br />
θ가 최선인가???를 확인하고,<br />
최선이 아니라면 다시 1번 과정으로 돌아간다.<br />
===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-019.png" /><br />
data : yn에 대해, zn = k에 대해 -&gt; reponsiblity = rnk<br />
일단 responsibility의 정의는 주어진 해당 데이터와, 현재의 가중치에 대해서<br />
zn = k 일 확률이다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-025.png" /><br />
definition of Responsibility<br />
Bayes rule을 이용하면,<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-026.png" /><br />
여기서 분모는 모든 latent variable의 z = 1 -&gt; K의 p(y|theat)의 합으로 표현가능하다.<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-027.png" /><br />
따라서 정리하면,<br />
<img alt="Ch3 Probability - Multi model - GMM" src="./images/img-019.png" /><br />
data : yn에 대해, zn = k에 대해 -&gt; reponsiblity = rnk</p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim</p>
      </footer>
    </main>
  </div>
  <script src="../../../../assets/js/main.js"></script>
</body>
</html>