<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ch3 Probability - part1 - Multivariant Gaussian Distribution | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../../assets/css/style.css">
</head>
<body>
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../../assets/images/bg.jpg" alt="Background" onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo" src="../../../../assets/images/profile.jpg" alt="Sehyeog Kim"
             onerror="this.style.background='#eaeef2'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Personal Blog</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="15" height="15" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-group-label">AI</span>
        <a href="/blog/ai/agentic-ai-theory/" class="nav-item">Agentic_AI_Theory<span class="nav-post-count">8</span></a>
        <a href="/blog/ai/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/ai/machine-learning/" class="nav-item active">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/ai/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <span class="nav-group-label">BioMechanics</span>
        <a href="/blog/biomechanics/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/biomechanics/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <span class="nav-group-label">Mechanical_Engineering</span>
        <a href="/blog/mechanical-engineering/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/mechanical-engineering/computational-fluid-dynamics/" class="nav-item">Computational_Fluid_Dynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/mechanical-engineering/engineering-mathematics/" class="nav-item">Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/mechanical-engineering/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/mechanical-engineering/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/mechanical-engineering/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/mechanical-engineering/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/mechanical-engineering/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <main class="main-content">
      <div class="breadcrumb"><a href="/">Home</a><span class="sep">/</span><a href="/blog/ai/">AI</a><span class="sep">/</span><a href="/blog/ai/machine-learning/">Machine_Learning</a><span class="sep">/</span><span>ch3 Probability - part1 - Multivariant Gaussian Distribution</span></div>
<a href="/blog/ai/machine-learning/" class="back-link">&larr; Back to Machine_Learning</a>
<div class="page-header"><h1>ch3 Probability - part1 - Multivariant Gaussian Distribution</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-12</span><span class="meta-item"><span class="meta-label">Category:</span> Machine_Learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/231" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>바로 본론으로 들어가자.<br />
어떤 서로다른 변수들의 분포를 나타내는 기본적인 도구가 바로<br />
Covariance(공분산)<br />
이다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-001.png" /><br />
Definition of Covariance.<br />
이제 변수가 벡터의 형태라면, 다음과 같이<br />
Covariance Matrix, sigma<br />
를 정의한다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-002.png" /><br />
즉, 쉽게 생각해서 Covariance는 행렬인데,<br />
i, j 번째 요소들의 공분산<br />
을 각각 저장해놓은 행렬이다.<br />
i, j 요소를 같이 측정했을때, 측정값으로 pair (i, j )가 나올 수 있는 확률이 (mean of i, mean of j ) 가 가장 높을 것이다.<br />
분포를 나타내면 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-003.png" /><br />
여기서 위 그래프의 퍼짐 정도를 우리는 cov(i,j)<br />
covariance of i, j 라고 부른다는 것.<br />
한편, covariance의 정의를 그대로 이용하면, 다음 식이 나온다.<br />
(추후에 쓰임)<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-004.png" /><br />
여기서, 이제 우리는 normalize 작업으로 X,Y의 원래의 공분산으로 나누어 주면,<br />
우리는<br />
correlation Matrix<br />
를 얻을 수 있다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-005.png" /><br />
즉, 원래 X,Y의 분포를 기준으로 공통분포는 얼마나 퍼졌는지를 나타내는 것이다.<br />
(standard normal distribution과 동일하다, 비교하기쉬워짐)<br />
Correlation을 직관적으로 이해하기 위해서, 아래그림을 살펴보자.<br />
Corr(x,y) 어떤 두 변수의 값을 plot하고, 상관관계를 구한 그림.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-006.png" /><br />
딱봐도 직선이면 1, 직선이 아니면 0. 그 사이는 0 ~ 1 로 표현된다.<br />
다르게 말하면,<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-007.png" /><br />
X,Y가 직선관계인지 아닌지를 나타내는 도구라는 것.<br />
이제 똑같이 corr( vector x)를 살펴보자.<br />
위에서 우리가 정의한대로 행렬을 구성하면 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-008.png" /><br />
이를 수학적으로 표현하면 다음과 같다.<br />
(수학은 단순하게 표현하는 도구일 뿐)<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-009.png" /><br />
Kxx는 사실 우리가 이미 정의한 Matrix인 Covariance Matrix이다.<br />
공분산으로 구성된 Matrix(Kij = Cov(i,j))<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-010.png" /><br />
Auto - Covariance Matrix. same as sigma<br />
diagonal term과 역행렬을 나타내면 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-011.png" /><br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-012.png" /><br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-011.png" /><br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-012.png" /><br />
따라서, Correlation Matrix는 다음과 같음을 증명가능하다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-009.png" /><br />
여기서 주의할 점은 X,Y가 선형관계 -&gt; large Correlation(X,Y)<br />
근데 그렇다고 X가 Y의 원인 혹은 Y가 X의 원인 인 것은 아니다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-013.png" /><br />
가장 대표적인 예시로 Violent crime과 Ice cream sale을<br />
월별로 나타내면 아래와 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-014.png" /><br />
Corrleation이 1에 굉장히 가까울 것이다.<br />
(선형관계이므로, in other words, 아이스크림 세일이 증가하면, 범죄도 증가/ 감소하면 같이 감소)<br />
하지만 우리는 두개의 변수 사이에 아무런 원인관계가 없다는 것을 알고,<br />
날씨가 원인이라는 사실을 알고 있다.</p>
<h1>Simpson's Paradox</h1>
<p><img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-015.png" /><br />
(확률하면 무조건 등장하는 역설임.)<br />
예시로 바로 위 역설을 이해해보자.<br />
대학 입시에서 “여자 지원자보다<br />
남자 지원자의 합격률이 더 높다<br />
”는 전체 통계가 나왔다고 가정하자.<br />
그런데 모든학과의 평균으로 살펴보니 사실 여자 지원자의 합격이 더 높게 나왔다.<br />
그 이유는, 저 통계는 알고보니 경쟁력이 높은 학과를 기준으로 측정한것.<br />
그리고 보통 경쟁력이 높은 학과는 (의대, 공대 등 남자 지원율자체가 앞도적으로 높은 곳)<br />
즉 하고싶은 말은 조사하는 Pool이 어디이냐에 따라서 공분산이 아예 달라진다는 것이다.<br />
공분산을 경제학과 pool안에서 측정하냐, 기계공학에서 측정하냐, 의학과에서 측정하냐 에따라서<br />
여자와 남자의 합격률 분포는 아예 다를 것이다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-016.png" /><br />
Pool z 를 기준으로 측정한 X,Y의 공분산.<br />
=====================================================================<br />
지금까지 다변수의 분포를 정의하기 위한 도구들을 배웠고,<br />
이제 실제로 어떤 분포를 띄는지 함수로 표현해보자.<br />
가장 기본적인 분포가 Gaussian Distribution (normal ditribution)이다<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-017.png" /><br />
X,Y의 Guassian Normal ditribution<br />
즉, X,Y각각 정규분포를 따르며 Probability density Function을 나타내면 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-018.png" /><br />
u: mean vector, sigma : Covariance Matrix, D: dimension<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-019.png" /></p>
<h1>Mahalnobis Distance</h1>
<p>Gaussian PDF에 log를 취해주자.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-020.png" /><br />
여기서 가운데 term을 우리는 Mahalnobis distance라고 정의하자.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-021.png" /><br />
갑자기 왜 이거를 무슨의미가 있길래 정의했을까?<br />
의미를 찾기 위해서 먼저 Sigma: Covariance Matrix를 Eigen Value Decomposition을 해주자.<br />
(D x D Matrix - square, symmetric, Semi positinve Definite Matrix이므로 EVD가능)<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-022.png" /><br />
재밌는 사실은<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-023.jpg" /><br />
따라서, Inverse of Covariance Matrix를 다음과 같이 표현가능하다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-024.png" /><br />
이제 우리가 위에서 정의한<br />
Mahalnobis Distance에 대입<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-025.jpg" /><br />
U =. u^T이 row vector인<br />
즉 y -&gt; z 벡터로 전환하게 되면, zd^2 Euclidean Distance를 의미하게 된다.<br />
z = U^T (y - u)<br />
y -&gt; z is mapping도대체 어떻게 변환이 되었는지를 보면,<br />
평균만큼 평행이동하고, U를 앞에다 곱한 것은, 벡터를 회전시킨 것이다.<br />
(정확히 표현하면, U의 열벡터를 표준기저로 하는 좌표공간으로 mapping한 것)<br />
(mapping의 자세한 의미는 아래 블로그 글에 담겨 있습니다. tensor의 mapping)<br />
https://jeffdissel.tistory.com/230<br />
5. Singular Value decomposition(SVD) - part2<br />
정말 중요하고 또 중요한 내용이라 part2에서 더 깊숙히 들어가 이해해보자. Eigen value decompositon(EVD)Singular Value decomposition(SVD)이 두가지는 정말 거의 모든 영역에서 쓰인다. 머신러닝의 회귀모델,<br />
jeffdissel.tistory.com<br />
이후에, eigen value로 나누어 주는 것이 elongation까지 진행.<br />
2D를 예시로 들면, Mahalnobis distance는 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-026.png" /><br />
정말 재미있는 사실은 위 distance가 정해지면, Probability 도 정해진다는 사실이다.(일대일 대응임)<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-020.png" /><br />
따라서, 위 타원위에 있는 (z1,z2) 점은 모두 같은 확률을 가진다.</p>
<h1>Marginals of MVN</h1>
<p>우리가 두개의 변수에 대한 MVN을 다음과 같이 가지고 있다고 가정하자.<br />
지금까지 배운 mean vector, covariance Matrix는 다음과 같다.<br />
여기서 Λ를 inverse of Covariance Matrix이며 다음과 같이 정의하자.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-027.png" /><br />
Λ =. Σ −1<br />
(Σ is a symmetric positive semi definite matrix,<br />
which has full rank leading to be a non singular Matrix that has inverse Matrix)<br />
그리고 Marginals of x는 다음과 같이 정의된다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-028.png" /><br />
쉽게 말해서, y 변수를 고려하지 않고, 순수한 x의 분포를 보는것.<br />
재미있는 사실은, MVN(<br />
Multivariance Normal distribution)의 핵심은<br />
변수 각각 개별도 normal distribution을 따른 다는 점이었다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-029.png" /><br />
Σ11 : variance of y1</p>
<h1>Conditionals of MVN</h1>
<p>MVN의 두번째 특징은 y2가 특정값이라고 가정했을때, 조건부 확률도 정규분포를 따른다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-030.png" /><br />
그 정규분포의 평균과 분산을 계산해서 구해보자.</p>
<h1>Schur Complement</h1>
<p><img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-031.jpg" /><br />
(계산하기 위한 도구, 선형대수학에서 정의)<br />
정의대로, MVN을 먼저 다음과 같이 expotential function으로 나타낼 수 있다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-032.png" /><br />
이후에 Covariance Matrix를 shur complement를 활용하여, 다음과 같이 쪼개주자.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-033.png" /><br />
쪼개준 이유는 조건부 확률을 만들어 내기 위해서다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-034.png" /><br />
쪼갠 식을 정리하면 위와 같으며, 뒤에 term은 normal distribution of variable x2임을 알 수 있다.<br />
여기서 bayes rule을 활용하면 우리는 조건부확률의 pdf를 구할 수 있는 것.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-035.png" /><br />
따라서, 정규분포의 평균과 분산은 다음과 같다.<br />
<img alt="ch3 Probability - part1 - Multivariant Gaussian Distribution" src="./images/img-036.png" /></p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim</p>
      </footer>
    </main>
  </div>
  <script src="../../../../assets/js/main.js"></script>
</body>
</html>