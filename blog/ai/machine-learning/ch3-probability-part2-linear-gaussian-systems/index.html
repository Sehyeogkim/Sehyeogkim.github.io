<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ch3 probability - part2, Linear Gaussian systems | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../../assets/css/style.css">
  <script>!function(){var t=localStorage.getItem("theme")||"light";document.documentElement.setAttribute("data-theme",t)}();</script>
</head>
<body>
  <button class="theme-toggle" aria-label="Toggle theme"><svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg><svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg></button>

  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../../assets/images/bg.jpg" alt="Background" onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo" src="../../../../assets/images/profile.jpg" alt="Sehyeog Kim"
             onerror="this.style.background='#eaeef2'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Personal Blog</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="15" height="15" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-group-label">AI</span>
        <a href="/blog/ai/agentic-ai-theory/" class="nav-item">Agentic_AI_Theory<span class="nav-post-count">8</span></a>
        <a href="/blog/ai/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/ai/machine-learning/" class="nav-item active">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/ai/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <span class="nav-group-label">AI_Application</span>
        <a href="/blog/ai_application/claude/" class="nav-item">Claude<span class="nav-post-count">2</span></a>
        <span class="nav-group-label">BioMechanics</span>
        <a href="/blog/biomechanics/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/biomechanics/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <span class="nav-group-label">Mechanical_Engineering</span>
        <a href="/blog/mechanical-engineering/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/mechanical-engineering/computational-fluid-dynamics/" class="nav-item">Computational_Fluid_Dynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/mechanical-engineering/engineering-mathematics/" class="nav-item">Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/mechanical-engineering/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/mechanical-engineering/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/mechanical-engineering/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/mechanical-engineering/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/mechanical-engineering/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/mechanical-engineering/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <main class="main-content">
      <div class="breadcrumb"><a href="/">Home</a><span class="sep">/</span><a href="/blog/ai/">AI</a><span class="sep">/</span><a href="/blog/ai/machine-learning/">Machine_Learning</a><span class="sep">/</span><span>ch3 probability - part2, Linear Gaussian systems</span></div>
<a href="/blog/ai/machine-learning/" class="back-link">&larr; Back to Machine_Learning</a>
<div class="page-header"><h1>ch3 probability - part2, Linear Gaussian systems</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-12</span><span class="meta-item"><span class="meta-label">Category:</span> Machine_Learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/m/232" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>ch3 probability - part2, Linear Gaussian systems<br />
(시작하자마자 상황극을 해보자)<br />
우리가 공장에서 센서로<br />
측정한 결과값(y)<br />
으로,<br />
제품의 완성도(z)<br />
를 예측하는 모델을 계발한다고 가정하자.<br />
여기서 측정하는 값과 완성도는<br />
Linear relationship<br />
이라고 가정하자.<br />
(즉 ,센서 측정값이 크면, 완성도가 높은 관계)<br />
하지만, 통신의 시간차, 센서의 진동 등등으로<br />
측정값에는 오차가 분명히 존재한다<br />
여기서,<br />
Error: e<br />
라고 정의하자.<br />
자 우리가 지금 두변수 z, e 확률분포를 안다고 가정하자.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-001.png" /><br />
- z의 가우시안 분포,<br />
- y 는 z 에 선형적으로 의존 + 잡음<br />
(which follows Gaussian distribution)<br />
((y = Wz + b + e), e is a perturbation)<br />
여기서 noise, e is orthogonal to z 라는 말이 이해가 안가지만,<br />
다시 표현하면 covariance = 0 이라는 뜻이다.<br />
서로 연관이 없는 변수라는 것<br />
(Independent)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-002.png" /><br />
(즉 잡음의 증가와 감소가 우리의 선형시스템의<br />
결과에 영향을 미치지 않는다고 가정하는 것이다)<br />
위 분포를 나타내면, 아래와 같다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-003.png" /><br />
우리가 위 두 정보를 가지고, p(y,z)를 구해보자.<br />
먼저 평균은 다음과 같다. (E(e) = 0)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-004.png" /><br />
이제 covariance Matrix만 구하면 된다.<br />
Matrix안의 요소를 각각 구해버리자.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-005.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-006.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-005.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-006.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-007.png" /><br />
따라서, 최종 covariance Matrix는 다음과 같다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-008.png" /><br />
지금까지 우리는 p(z,y)의 Mean vector and Covariance Matrix를 유도하였다.<br />
이제<br />
P(z|y)를<br />
구해보자. (왜 구하는지는 이후에 설명)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-009.png" /><br />
두 식을 더하면, 우리는 bayes rule을 활용하여<br />
p(z|y)<br />
를 구할 수 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-010.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-011.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-010.png" /><br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-011.png" /><br />
bayes rule<br />
생각보다 그렇게 지저분하지는 않다. 더해주자.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-012.png" /><br />
더해서 새로운 gaussian 정규분포 Form으로 전환하여, 평균과 분산을 구하는게 목표이다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-013.png" /><br />
두번째 식이 가우시안 분포의 likelihood term이고 이를 만족하려면 평균은 아래와 같아야 함을 알 수 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-014.png" /><br />
따라서, 기호로 표현하면 아래와 같이 평균과 분산으로 나타낼 수 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-015.png" /><br />
====================================================================<br />
우리가 무엇을 구하고자 하는 지를 정확히 정의하고 가자.<br />
예시로 다음과 같이 데이터 y와 예측하고 싶은 z를 정의하자.<br />
y= 환자의 검사 결과, z: 환자 질병상태 (암 yes or no).<br />
Prior p(z):<br />
우리가<br />
관찰하기 전에<br />
latent 변수<br />
z<br />
에 대해 가지고 있는 믿음(사전 지식).<br />
(보통 인간이 암이 일어날 확률)<br />
Likelihood p(y∣z):<br />
주어진<br />
z<br />
에서 실제 관측<br />
y<br />
가 얼마나 그럴듯한지 (실험 데이터로부터 얻는 정보).<br />
(암인 환자와 아닌 환자를 비교했을때 환자의 검사결과)<br />
Posterior p(z∣y)<br />
(우리가 알고싶은 데이터, 환자 검사결과를 기준으로 암 확률)​<br />
즉 우리는<br />
z,y관계는 Gaussian Linear system라는 가정에서 출발하여<br />
p(z|y)를 구하는 것이 최종목적<br />
(따라서 위에서 유도함)<br />
== == == == == == == == == == == == == == == == == == == == == == == == == == ==<br />
이차방정식을 완전제곱꼴로 묶어주면 다음과 같다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-016.png" /><br />
이제 우리는 Matrix equation 을 함유하는 vector function을 나타내면 다음과 같다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-017.png" /><br />
같은 원리로, 위와같이 완전제곱꼴로 묶어주면 다음과 같이 나타낼 수 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-018.png" /><br />
(꼭 기억해두자!!! 추후에 계속해서 쓰임)<br />
자 이제 실제로 위에서 증명한 식들이 어떻게 쓰이는지를 적용해보자.<br />
Example1: Infering an unknwon scalar<br />
Infer an<br />
unknown scalar value z<br />
. We have<br />
N<br />
noisy measurements<br />
y i ​<br />
.<br />
Each measurement is assumed to follow a<br />
Gaussian likelihood<br />
:<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-019.png" /><br />
우리에게 주어진 likelihood<br />
그리고 Prior has<br />
Gaussian distribution 가정하자.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-020.png" /><br />
우리에게 주어진 Prior<br />
우리가 배운 위 bayes rule을 통해서 Posterior distribution is Gaussian임을 우리는 알고 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-021.png" /><br />
우리가 위에서 배운, y = Wz + b 에서<br />
(W를 그냥 1벡터라고 아래처럼 가정)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-022.png" /><br />
위에서 증명한 식에 그대로 대입하면,<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-023.png" /><br />
Posterior mean and covariance that we derived previously<br />
다음의 mena and precision 값이 나오게 된다(scalar)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-024.png" /><br />
Example2: Infering an unknwon vector<br />
이번에는 벡터 z를 추론하는 경우 posteir mean and covariance를 구해보자.<br />
여기서 햇갈리면 안되는게 벡터 z를<br />
( D x 1 ) 차수라고 하자.<br />
예를들어<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-025.png" /><br />
이렇게 한 환자당 3개의 데이터를 측정했다면 D = 3이고 yn안에 3개의 정보가 저장되어있는 것.<br />
여기서 N은 그러한 환자가 몇명이냐는 것이다.<br />
즉 데이터의 숫자를 의미한다.<br />
위 개념을 가지고 들어가보자.<br />
먼저 prior distribution is Gaussian이라는 가정으로 시작한다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-026.png" /><br />
prior of vector z<br />
그리고<br />
N<br />
independent noisy measurements<br />
(<br />
y = wz + b + e = z + e)<br />
여기서 우리는 측정한 N개의 데이터 모두 각각<br />
독립적으로<br />
Gaussian distribution<br />
을 따른다고 가정하자.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-027.png" /><br />
W = 1, b = 0 인 경우<br />
여기서 우리가 구하고싶은<br />
Likelihood<br />
는<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-028.png" /><br />
"만약 진짜 값이<br />
z<br />
라면, 현재 우리가 관측한 전체 데이터셋<br />
D={y1,…,yN}<br />
이 나올 확률"<br />
을 의미한다.<br />
따라서, P(D|z)<br />
는 전체 N개의 데이터 pdf각각의 곱과 같다 .<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-029.png" /><br />
Likelihood for all measured data.<br />
여기서 log함수를 취해주자. (곱셉 -&gt; 덧셈)<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-030.png" /><br />
한 데이터에서 N개의 y의 평균을 해주면,<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-031.png" /><br />
이제 위 y bar를 적용하면 우리는 normal distribution form으로 전환이 가능하다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-032.png" /><br />
따라서, 전체 data의 likelihood는 아래와 같이 나타낼 수 있다.<br />
<img alt="ch3 probability - part2, Linear Gaussian systems" src="./images/img-033.png" /></p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim</p>
      </footer>
    </main>
  </div>
  <script src="../../../../assets/js/main.js"></script>
</body>
</html>