<!DOCTYPE html>
<html lang="ko" data-theme="dark">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4. Eigen Value Problems - Part1 | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
  <script>
    (function(){var t=localStorage.getItem('theme')||'dark';document.documentElement.setAttribute('data-theme',t)})();
  </script>
</head>
<body>
  <!-- Theme toggle -->
  <button class="theme-toggle" aria-label="Toggle theme">
    <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
    <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
  </button>

  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Personal Blog</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item active">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/computational-fluid-dynamics/" class="nav-item">Computational_Fluid_Dynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/engineering-mathematics/" class="nav-item">Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/machine-learning/" class="nav-item">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/computational-linear-algebra/">Computational-Linear-Algebra</a><span class="sep">/</span>  <span>4. Eigen Value Problems - Part1</span></div>
<a href="/blog/computational-linear-algebra/" class="back-link">&larr; Back to Computational-Linear-Algebra</a>
<div class="page-header"><h1>4. Eigen Value Problems - Part1</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-07</span><span class="meta-item"><span class="meta-label">Category:</span> Computational-Linear-Algebra</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/m/226" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><ol>
<li>Eigen Value Problems - Part1<br />
선형대수학에서 무조건 나오는 내용이고,<br />
처음에 이해하기 어려운 내용이 바로<br />
"고유값,고유벡터,고유값 분해"<br />
개념이다.<br />
다시한번, 빠르게<br />
리뷰<br />
를 해보도록 하자.<br />
Square Matrix A<br />
가 존재한다고 하자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-001.png" /><br />
밑의 행렬 방정식의 해<br />
x<br />
를 우리는 고유벡터,<br />
그때의<br />
λ<br />
(scalar)를 고유값이라고 한다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-002.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-003.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-004.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-003.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-004.png" /><br />
(저게 왜 쓸모 있을까? 에. 대한 답변을 하기 위해)<br />
기하학적으로 무슨 의미인지를 이해해보자.<br />
위 식의을 다시 살펴보면,<br />
(2nd order tensor) * vector1 = vector2<br />
계속해서 언급하지만, tensor의 역할은 mapping이다.<br />
즉, vector1 -&gt; vector2로 mapping해주는 역할이다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-002.png" /><br />
다시 위 식을 보면, A는 mapping역할로 x를 전환해주었지만,<br />
크기만 다르고 같은 방향의 벡터로 mapping이 되었다.<br />
(so what?)<br />
즉 어떤 기저벡터가 2차원 평면에 있다고 했을때,<br />
기저벡터들을 이런식으로 scale만 바꿔주는 역할을 한다는 것.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-005.png" /><br />
수학적 정의에 대해서 정확히 기억하고 뒤에 이야기를 들어 가야한다.<br />
eigenspace, eigenvector,<br />
spectrum of A (eigen value 집합)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-006.png" /><br />
자 이제, 우리의 목표의 다음목표는<br />
A의 eigen value를 구하는 것.<br />
Ax = λx 를 만족하는 lambda : z라고 미지수로 설정하면,<br />
우리는 다음의 characteristic polynomial Eq 을 세울 수 있다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-007.png" /><br />
Polymoial Equation<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-008.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-009.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-008.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-009.png" /><br />
여기서 드는 질문은,<br />
왜 우리가 eigenValue를 구해야하는가????<br />
우리의 원래 목적은 Ax = b를 푸는 것인데??<br />
(라는 질문이 pop up 할 것이다.)<br />
바로 eigenvalue를 활용한, eigenvalue decomposition으로<br />
A를 분해 할 수 있기 때문이다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-010.png" /><br />
X: matirx composed with Eignen vectors<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-011.png" /><br />
따라서, 우리가 풀고자하는 문제를 다음과 같이 재 표현할 수 있다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-012.png" /><br />
여기서 우리가 새로운 y 백터를 다음과 같이 정의해주자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-013.png" /><br />
이후에 대입해주면, y를 쉽게 구할 수 있게 된다.<br />
(다시 x를 쉽게 구할 수 있음 위 식으로)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-014.png" /><br />
=================================<br />
but Here's a Problem.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-015.png" /><br />
우리가 중학교때 배웠던 근의 공식으로, 2차,3차 polynomial Eq의 해는 구할 수 있지만,<br />
4차부터는 일반적인 해의 form이 존재하지 않는다.<br />
(In other word, direct method to solve above Eq은 존재하지 않고,<br />
iterative Method를 사용해야한다는 것)<br />
따라서, 오늘은 Iterative Method로 어떻게 Eigenvalue를 구할 수 있는지,<br />
알고리즘들을 살펴보자.<br />
Let<br />
A<br />
∈<br />
C<br />
m<br />
×<br />
m<br />
.<br />
For simplicity, let’s assume A has a set of<br />
m<br />
linearly independent eigenvectors<br />
⃗<br />
v<br />
1<br />
, ⃗<br />
v<br />
2<br />
, ..., ⃗<br />
v<br />
m<br />
which must then form a basis for<br />
C<br />
m.<br />
그리고 eigen value들을 오름차순으로 정렬하여 index를 붙혀주자.<br />
|<br />
λ<br />
1<br />
| ≥ |<br />
λ<br />
2<br />
| ≥<br />
...<br />
≥ |<br />
λ<br />
m<br />
|<br />
여기서 새로운 정의가 나온다.<br />
If<br />
|<br />
λ<br />
1<br />
|<blockquote>
<p>|<br />
λ<br />
2<br />
|<br />
,<br />
λ1<br />
is called the dominant eigenvalue and<br />
v<br />
1<br />
is called a dominant eigenvector of A.<br />
갑자기 왜 가장큰 eigenvalue에 집중을 했는지를<br />
머리속에 넣고 밑의 글을 읽어보자.<br />
어떤 임의의 x벡터를 eigenvector들의 선형결합으로 표현하자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-016.png" /><br />
위식의 양변에 Matrix A를 곱해주고, Eigenvector의 성질을 이용해서 eigenvalue 식으로 치환해주자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-017.png" /><br />
여기서 계속해서 A를 곱하는 이유는, λ1 이 가장 크다는 사실을 이용해,<br />
곱하면 곱할수록,<br />
괄호안의 eigenvector들의 선형결합속에서 v1의 영향력이 더 커지게 된다<br />
( λk / λ 1 &lt;0 when k &gt;2, 나머지는 계수들이 점점 작아지므로)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-018.png" /><br />
다시 돌아가서 j번째 A를 곱했을때의 식 양변을<br />
λ<br />
1 j 로 나누어 주자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-019.png" /><br />
그리고 c1v1을 옆으로 넘겨주면, 좌항이 의미하는 것은 k번째 곱해진 벡터에서 v1성분을 제거한것.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-020.png" /><br />
위 식을 부등식으로 다음과 같이 표현가능하다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-021.png" /><br />
좌항이 정확히 어떤 의미를 함축하는 지를 정확히 이해해야한다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-022.png" /><br />
(j를 곱하면 곱할수록 v1성분만 남게 되므로, 위 식은 0에 가까워지게 된다)<br />
따라서, 잔차 residual이라고 할 수 있다.<br />
그리고 그 잔차는 비율에 비례한다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-023.png" /><br />
위 식을 다시 수학적으로 표현해보면, 가장큰 eigenvalue에 대응하는 eigenvector v1에 대해서<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-024.png" /><br />
정규화된 q1을 정의하면, k번 곱해진 vector vk는 결국 q1에 수렴할 것이고, 그 에러는 다음과 같이 표현된다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-025.png" /><br />
우리는 이 알고리즘을 Power method라고 부르며, Pseudo code는 다음과 같다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-026.png" /><br />
Residual 이 특정이하로 떨어질때까지, 계속해서 반복하여, A를 곱해준다.<br />
예시로 정확히 이해해보자.<br />
Power Method로 A matrix의 dominant eigen value 구하기.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-027.png" /><br />
x0 = [1, 1] T 부터 시작.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-028.png" /><br />
즉,<br />
λ1<br />
1st iteration = 10 이므로,<br />
10으로 다시 나누어 주자.<br />
그리고 다시 A를 곱해주자.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-029.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-030.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-029.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-030.png" /><br />
이제<br />
λ1<br />
2nd iteration = 9.3이므로,<br />
9.3으로 나누어 주고 다시 A를 곱해주자.<br />
쭉 진행해주면, Dominator(<br />
λ1<br />
) 과 x가 수렴함을 알 수 있다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-031.png" /><br />
=============================================<br />
위의 power method에서 한발짝만 더 나아가 보자.<br />
Power Method<br />
는 잘 생각해보면, Dominant Eigen Value (가장 큰 절댓값)과 그에 대응하는 고유벡터를 찾는 데 유용하다.<br />
하지만 실제로는 가장 큰 고유값 이 아니라, 어떤 특정한 값 μ 근처의 고유값과 고유벡터를 알고 싶은 상황이 존재.<br />
위의 경우 해결책:<br />
Inverse Iteration (Shifted Inverse Iteration)<br />
단순히 기존 characteristic Eq에서 양변을 u v로 빼주면,<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-032.png" /><br />
따라서, A가 역행렬이 존재하는 nonsingular square matrix라면, 다음 성질을 만족한다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-033.png" /><br />
위 성질을 그대로 power method에 이용하면, 우리는 가장 큰<br />
λ1<br />
을 이용한 powermethod에서<br />
우리가 원하는 특정한 값 mu 와. 가까운<br />
λ<br />
를 기준으로 powermethod의 변형을 적용가능.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-034.png" /><br />
Pseudo Code는 다음과 같고, 정확히 Power method 에서<br />
A -&gt;<br />
(A - μI)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-035.png" /><br />
=============================================<br />
정말 마지막으로<br />
Similiarity<br />
의 개념을 살펴보자.<br />
우리는 밑의 관계식을 만족한는 Matrix X가 존재하다면,<br />
"two matrices A and B are similar"<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-036.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-037.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-036.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-037.png" /><br />
근데 simliar 하다는게 왜 특별할까?<br />
"Similarity does not change the eigenvalues"<br />
—so we can replace a “hard” matrix by a<br />
simpler similar matrix<br />
(triangular, Hessenberg, tridiagonal, diagonal) and keep the spectrum.<br />
For instance, A와 X가 다음과 같다면,<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-038.png" /><br />
같은 Eigen value를 가지는 B를 만들 수 있고, Diagonal Matrix이므로, 이후 연산이 쉬워진다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-039.png" /><br />
한편, A와 B가 같은 Eigenvalue를 가지는 것은 다음과 같이 쉽게 증명할 수 있다.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-040.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-041.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-040.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-041.png" /><br />
그렇다면, eigen vector도 같은가??<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-042.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-043.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-042.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-043.png" /><br />
(X의 역행렬의 곱한 벡터로 mapping되는 것을 알 수 있다)<br />
=============================================<br />
자 이제 Similiar 조건을 다양하게 적용해보자.<br />
Definition: "two matrices A and B are similar"<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-036.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-037.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-036.png" /><br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-037.png" /></p>
</blockquote>
</li>
<li>Diagonalizable A<br />
만약 A가 m개의 독립적인 eigenvector(v1...vm) 를 가지고,<br />
그에 상응하는 eigenvalue로 diagonal Matrix를 만들면,<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-044.png" /><br />
우리는 다음과 같이 A,D가 Similiar Matrix라는 것을 알 수 있다.<br />
(B = D , X = V 인 상황)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-045.png" /></li>
<li>Unitary Simlilar (X = U)<br />
Similiar Matrix에서 X is unitary 라면, (U*U = I)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-046.png" /><br />
더해서, column Vector들이 서로 수직이라면 Q: Orthogonal Matrix<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-047.png" /><br />
Similiar Matrix에서 X is Orthogonal Matrix Q 라면,<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-048.png" /><br />
위 의경우, A와 B의 norm이 같기 때문에 수치적안정석으로 추후에 실제로 굉장히 많이 쓰임,<br />
(추후에 등장)</li>
<li>Spectral Theorm (Hermitian case)<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-049.png" /><br />
고유값분해와 동일한 원리이지만, Hermitan 인 경우는 U<em> = U-1이므로,<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-050.png" /><br />
(columns of<br />
U<br />
are orthonormal eigenvectors, entries of<br />
D<br />
are real eigenvalues)<br />
첫장에서 배웠지만, A</em> 는 Conjugate matrix로 complex number를 포함한 A의 대칭행렬이다.<br />
실수 영역에 대해서 우리는 symmetric이라고 부르며, 위의 Spectral decomposition을 다음과 같이 표현가능.<br />
<img alt="4. Eigen Value Problems - Part1" src="./images/img-051.png" /><br />
이 분해방법들을 다룬 이유는<br />
추후에 나올 Ax = b를 iterative method로 푸는 과정에서,<br />
연산을 쉽게해주는 방법론으로 사용되기 때문이다!!.</li>
</ol></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>