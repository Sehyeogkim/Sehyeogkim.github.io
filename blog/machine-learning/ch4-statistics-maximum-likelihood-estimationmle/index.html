<!DOCTYPE html>
<html lang="ko" data-theme="dark">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ch4 Statistics - Maximum Likelihood Estimation(MLE) | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
  <script>
    (function(){var t=localStorage.getItem('theme')||'dark';document.documentElement.setAttribute('data-theme',t)})();
  </script>
</head>
<body>
  <!-- Theme toggle -->
  <button class="theme-toggle" aria-label="Toggle theme">
    <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
    <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
  </button>

  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Personal Blog</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/computational-fluid-dynamics/" class="nav-item">Computational_Fluid_Dynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/engineering-mathematics/" class="nav-item">Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/machine-learning/" class="nav-item active">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/machine-learning/">Machine_Learning</a><span class="sep">/</span>  <span>Ch4 Statistics - Maximum Likelihood Estimation(MLE)</span></div>
<a href="/blog/machine-learning/" class="back-link">&larr; Back to Machine_Learning</a>
<div class="page-header"><h1>Ch4 Statistics - Maximum Likelihood Estimation(MLE)</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-13</span><span class="meta-item"><span class="meta-label">Category:</span> Machine_Learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/237" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>지금까지 우리는<br />
regression classification<br />
하기 위해서,<br />
확률모델을<br />
어떻게 설계해야하는 지<br />
모델에 집중했다면,<br />
이제 최적의 모델<br />
파라메터 θ<br />
를<br />
어떻게 찾아야 하는지를 살펴보자.<br />
정의는 다음과 같다.<br />
Maximum Likelihood Estimation (MLE)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-001.png" /><br />
단순하다,<br />
현재 가지고 있는 Data의 집합 D<br />
를 가장 높은 확률로 나타내는<br />
파라메터<br />
(<br />
θ)를<br />
설정하는 것.<br />
여기서, 데이터 집합에는 여러가지의 데이터들이 섞여 있다.<br />
각각 측정값은 독립적이므로 확률의 독립은 곱셈으로 표현된다.<br />
(여러개의 센서로 측정한 경우, 센서 각각은 독립적)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-002.png" /><br />
independent and identically distributed.<br />
여기서, log를 씌워주어 곱셈을 덧셈으로 전환해주고,<br />
log likelihood<br />
라고 정의하자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-003.png" /><br />
여기서, 우리는 결국 최적화를 진행하는 것이다. 가장 합이 크게하는<br />
θ<br />
를 찾는 것.<br />
(이후에 배우겠지만, 최적화를 위해서는 최솟값으로 만드는 것이 가장 좋다)<br />
따라서 -를 앞에 붙혀주어서,<br />
negative Log likelihood<br />
를 정의해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-004.png" /><br />
최솟값 문제로 전환.<br />
첫번째 방법은, prior p(<br />
θ<br />
) ~ 1 , 모든<br />
θ에 대한 사전지식이 없는 상황이라면<br />
우리는 간단하게 Bayes Rule을 이용해서 다음과 같이 표현가능하다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-005.png" /><br />
두번째 방법은, 현재 모델의.<br />
θ를 기준으로 데이터의 D의 확률분포 를 q(y)로 정의하고,<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-006.png" /><br />
우리가 training에 사용할 실제 데이터의 확률분포를 다음과 같이 p(y)로 표현하자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-007.png" /><br />
여기서 p(y)와 q(y)의 차이를 적게하는<br />
θ를 찾는 것이다.<br />
여기서 주의할점은 단순히 그 차이를 뺄셈식으로 표현하지 않고<br />
[KL divergence]<br />
을 이용한다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-008.png" /><br />
Equation of KL divergence<br />
여기서 좌측 H(p)는<br />
entropy of p<br />
여기서 우측 H(p,q)는<br />
cross-entropy of p and q<br />
주어진 데이터에 대해서 PD(y)는 고정된 값이고,<br />
불연속적인 데이터를 간접적 모델로 표현하면 다음과 같다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-009.png" /><br />
deltafunction을 활용하여, 우리는 전체 domain y에 대한 함수로 표현<br />
즉, KL divergence 식이 의미하는 것은.<br />
'현재 θ를 기준으로 한 모델의 데이터 분포와 실제 데이터 분포의 차이'<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-010.png" /><br />
따라서, 당연히 가장 작은 값을 가지는<br />
θ를 찾는 것이 우리의 목표.<br />
(최솟값 문제)<br />
자 이론식을 배웠고, 이제 여러 Distribution의 MLE를 구하는<br />
연습을 통해 감을 MLE가 무엇인지 감을 잡아보자.<br />
Example1: MLE of Bernoulli Distribution<br />
MLE식을 가장 간단한 베르누이 분포 에 적용해보자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-011.png" /><br />
Bernoulli Distribution<br />
Negative Log Likelihood는 정의에 따라 다음과 같이 나타낼 수 있다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-012.png" /><br />
편의상, N1, N0를 다음과 같이 정의하자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-013.png" /><br />
그리고 최적의<br />
θ<br />
를 찾기 위해서 미분을 해주면,<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-014.png" /><br />
미분 = 0. 일대가 극솟값이므로, 그 지점은 MLE는 다음과 같다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-015.png" /><br />
Example2: MLE of Categorical Distribution<br />
베르누이 분포 다음으로 categorical 분포로 MLE를 구해보자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-016.png" /><br />
Definition of categorical distribution<br />
기호로는 다음과 같이 표기한다. 즉, Y 각각의 확률을<br />
θ<br />
로 표기한다는 것이다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-017.png" /><br />
우리가 (1~K)의 숫자가 적혀 있는 주사위를 N번 던졌다고 해보자.<br />
여기서, 눈금 k가 나온 횟수를 Nk라고 정의하고,<br />
그 나온 n번째 던졌을때 결과를 Yn이라고 하자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-018.png" /><br />
그 확률을 categorical distribution이라는 가정을 하면<br />
우리는 NLL을 다음과 같이 표기할 수 있다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-019.png" /><br />
여기서 그냥 미분을 해서.<br />
θ_MLE<br />
를 구하고 싶지만, 한가지 제약조건이 또 걸려있다.<br />
(sum of all<br />
θ<br />
= 1)<br />
따라서, 이런 경우 우리는 Largangian Multiplier를 사용하여 다음과 같이 새로운 함수로 정의한다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-020.png" /><br />
이제 미분을 해주면,<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-021.png" /><br />
여기서 lambda는 Boundary Condition으로 쉽게 구할 수 있다.<br />
(전체 던진 횟수 =. 각 주사위 인덱스가 나온 횟수의 합)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-022.png" /><br />
따라서, MLE 는 다음과 같다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-023.png" /><br />
즉, k index가 주사위에 나올 확률은,<br />
우리가 던진 총 횟수와, k가 나온 비율로 계산된다.<br />
(empirical probability와 정확히 동일)<br />
Example3: MLE of Gaussian Distribution<br />
이번에는 총 N개의 데이터 각각이 Gaussian Distribution을 따르는 경우를 살펴보자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-024.png" /><br />
그리고 모델을 구성하는<br />
θ를 가우시안 분포의 평균과 분산으로 정의한다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-025.png" /><br />
따라서, Negative Log Likelihood를 다음과 같이 Normal Distribution으로 표기 가능하다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-026.png" /><br />
정리하면, 가우시안 확률분포 식을 대입해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-027.png" /><br />
stationary point를 찾기 위해 평균과 분산으로 미분을 해주면,<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-028.png" /><br />
다음과 같이 식이 MLE<br />
θ<br />
(평균과 분산)을 구할 수 있다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-029.png" /><br />
위에서 s^2의 정의는 다음과 같다<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-030.png" /><br />
즉, 우리는 가지고 있는 데이터로 충분히 MLE를 구할 수 있게 된다.<br />
Example4: MLE of Multivariant Gaussian Distribution<br />
ch3에서 다룬 다변량 가우시안 분포 pdf식을 떠올려 보자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-031.png" /><br />
pdf 식을 이용하면, we can easily get the log likelihood<br />
(when<br />
θ<br />
is the mean vector and covariance Matrix)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-032.png" /><br />
먼저 MLE mean vector를 구해보자.<br />
(구하기 전에 Quaratic matrix form 의 미분식을 Linear algebra책에서 꺼내오자)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-033.png" /><br />
그리고 yn - u 를 zn으로 치환해주고, 미분을 진행해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-034.png" /><br />
그리고, 이제 derivative Loglikelihood wrt/ u를 해주면, 우리는 MLE mean을 구할 수 있다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-035.png" /><br />
이제 Covariance Matrix at MLE를 구해보자.<br />
(여기서도, 구하기전에 scalar항을 Trace로 전환하고 시작하자.)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-036.png" /><br />
따라서, Log likelihood는 다음과 같이 trace로 표현가능하다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-037.png" /><br />
Tr(ABC) = Tr(ACB) 안에서 자유롭게 순서 바꾸기 가능.<br />
따라서, Precision = inverse of Convariance Matrix임을 자유롭게 이용하면,<br />
MLE에서의 Covariance를 다음과 같이 유도가능하다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-038.png" /><br />
Example5: MLE of Linear Regression.<br />
오늘의 마지막으로 Linear regression일때의 pdf를 loglikelihood에 대입해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-039.png" /><br />
pdf를 알고 있으니, 그대로 Negative LogLikelihood를 구해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-040.png" /><br />
그 다음, NLL에서 필요없는 상수항을 제거하고, 변수항만 남기면<br />
Residual sum of squares(RSS)<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-041.png" /><br />
여기서 평균으로 구해주기위해 총 데이터 수 N으로 나누면, Mean Sqaure error(MSE)가 등장한다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-042.png" /><br />
여기에 root를 씌워주면, 자주등장하는 Root Means Square Error(RMSE)가 된다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-043.png" /><br />
이제 MLE parameter w를 구하기 위해서, RSS를 Matrix form으로 전환해주자.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-044.png" /><br />
즉, 우리가 찾고 싶은 것은 stagnation point가 되는 w를 찾고 싶고, 그때의 w가<br />
Most likelihood estimation of w.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-045.png" /><br />
Linear Algebra의 Matrix derivative를 사용하면 다음과 같이 구할 수 있게 된다.<br />
<img alt="Ch4 Statistics - Maximum Likelihood Estimation(MLE)" src="./images/img-046.png" /></p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>