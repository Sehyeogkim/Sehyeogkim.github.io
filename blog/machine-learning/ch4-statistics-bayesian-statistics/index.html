<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ch4 Statistics - Bayesian Statistics | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
</head>
<body>
  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Knowledge Base</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/advanced-engineering-mathematics/" class="nav-item">Advanced_Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/machine-learning/" class="nav-item active">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/numerical-heat-transfer-and-fluid-flow/" class="nav-item">Numerical-Heat-transfer-and-Fluid-flow<span class="nav-post-count">14</span></a>
        <a href="/blog/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/machine-learning/">Machine_Learning</a><span class="sep">/</span>  <span>Ch4 Statistics - Bayesian Statistics</span></div>
<a href="/blog/machine-learning/" class="back-link">&larr; Back to Machine_Learning</a>
<div class="page-header"><h1>Ch4 Statistics - Bayesian Statistics</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2025-09-14</span><span class="meta-item"><span class="meta-label">Category:</span> Machine_Learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/239" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>Ch2에서 다루었지만, 확률의 개념은 Two Concepts.<br />
1. Frequentist<br />
확률 = 무한 반복 실험에서 사건이 일어나는<br />
장기 상대도수(long-run frequency)<br />
예: 동전을 무한히 던졌을 때 앞면이 나올 비율이 0.5라면, 그게 “확률 0.5”의 의미.<br />
파라미터(θ)는 고정된 값<br />
, 확률 개념을 적용하지 않음.<br />
2. Bayesian<br />
확률 = 어떤 사건이나 파라미터에 대한 <strong>불확실성(uncertainty)</strong>을 수치로 표현한 것.<br />
파라미터 θ 자체도 확률변수로 취급.<br />
데이터를 보기 전에는<br />
prior, p(θ)<br />
로 불확실성을 표현<br />
데이터를 본 후에는<br />
posterior p(θ∣D)<br />
로 업데이트.<br />
즉, Frequentist는 prior이란 개념을 사용하지를 않고,<br />
그냥 Data D를 가지고 확률을 정의한다.<br />
(무한히 동전을 던졌을때 앞면이 나온 횟수 / 전체 던진 횟수 = 앞면이 나올 확률)<br />
여기서 Bayesian은 Prior의 개념을 이용하고, Dataset과 의 조합으로 Posterior을 구한다.<br />
쉽게 말해,<br />
동전의 앞면이 나올 확률 = 1/2 라는 사전 믿음을 전제<br />
하고,<br />
앞으로의 data를 추가하여, 그 posterior를 구하는 것.<br />
여기서 Bayesian은 아주. 아주 큰 장점이 있다.<br />
우리가 어떤 동전을 던졌고,<br />
그 동전이 앞인지 뒤인지 예측하는 모델을 구했다고 해보자.<br />
여기서 모델은, 앞면/뒷면 인지는 예측 할 수 있지만,<br />
그 예측이 얼마나 불확실한지는 알려주지 않는다.<br />
In statistics, modeling uncertanity about parameters using a<br />
probability distribution -&gt; is known as<br />
Inference<br />
.<br />
여기서 우리는 Posterior Distribution을 활용하여, Uncertanity를 구할 것.<br />
Posterior을 구하는 식은 다음과 같다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-001.png" /><br />
Prior<br />
= 데이터 이전의 믿음<br />
Likelihood<br />
= 특정<br />
θ<br />
가 데이터와 얼마나 잘 맞는지<br />
(이것만 기억하자, Prior 데이터를 반영하기 이전의 확률값이 - prior)</p>
<h1>Conjugate Prior</h1>
<p><img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-002.png" /><br />
if<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-003.png" /><br />
(F: type of Expotential Family, 아래 링크에서 자세히 설명)<br />
https://jeffdissel.tistory.com/233<br />
쉽게 말해 prior과 Posterior 두개 확률 분포 form이 같다면 -&gt; conjugate prior<br />
동전 던지기로 예를 들어보자.<br />
총 N번 동전을 던졌다. n번째 사건에서<br />
yn = 1 이면 동전이 head,<br />
yn = 0 이면 동전이 tail<br />
N번 던지고 총 결과를 Dataset D에 다음과 같이 저장.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-004.png" /><br />
여기서 확률은<br />
Bernoulli Distribution<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-005.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-006.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-005.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-006.png" /><br />
theata: 앞면이 나올 확률<br />
여기서, 매 사건은 독립적이므로,<br />
likelihood<br />
는 다음과 같이 정의된다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-007.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-008.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-007.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-008.png" /><br />
여기서 새로운 모델의 정의를 살펴보자.<br />
Binomail model - N번 던졌을때 y번 head가 나올 확률.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-009.png" /><br />
자, 여기서 우리는<br />
Conjugate prior<br />
이라고 가정하여,<br />
즉, 다음과 같이 Likelihood와 같은 form이라고 하자 (Beta distribution form)<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-010.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-011.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-010.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-011.png" /><br />
ch2에서 다룬, Definition of Beta distribution<br />
따라서,<br />
prior * likelihood -&gt; posterior<br />
이므로, posterior이 다음의 beta분포임을 알 수 있다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-012.png" /></p>
<h1>pos terior predictive</h1>
<p>이제 주어진 데이터로 new y를 띌 확률이 얼마인지 계산하는 것.<br />
위 Bernoulli model같은 경우 우리는, posterioral, 그리고 beta distribution을 알기 때문에,<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-013.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-014.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-013.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-014.png" /><br />
Note. 동전 앞이 나올 확률이므로 그냥 theat이다.</p>
<h1>Mixtures of conjugate priors</h1>
<p><img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-015.png" /><br />
이전까지 우리는 prior을 하나의 beta함수로 표현한 예시만 들었지만,<br />
gaussian subgroup으로 나누어 그 합으로 표현하듯이,<br />
conjugate prior도 여러개의 합으로 위와 같이 표현할 수 있다.</p>
<h1>Dirichlet-multinomial model</h1>
<p>지금까지는 베르누의 모델만 봤다면, 이제는 주사위와 같은 categorical distribution<br />
을 살펴보자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-016.png" /><br />
주사위 면 c개 있고, index: c가 나올 확률 theat_c<br />
여기서 likelihood는 독립적인 확률을 곱해주어 표현하자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-017.png" /><br />
우리는 likelihood와 동일한 expoential family에 있는 prior을 설정해야한다.<br />
따라서, likelihood가 속해있는<br />
Dirichlet Distribution<br />
의 정의를 살펴보자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-018.png" /><br />
위 정의에 필요한,<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-019.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-020.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-019.png" /><br />
Probability Simplex<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-020.png" /><br />
multivariate beta function<br />
이제 posterior을 likelihood와 prior의 곱인 다음의 표현으로 살펴보자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-021.png" /></p>
<h1>Gaussian - Gaussian Model (univariant)</h1>
<p>이번에는 가우시안 분포를 살펴보자, univariate일때 Likelihood는 다음과 같다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-022.png" /><br />
여기서 같은 family로 prior을 정의하면, 다음과 같이 평균과 분산을 미지수로 설정하자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-023.png" /><br />
prior Gaussian Distribution<br />
이때, 두 pdf의 곱으로 새로운 Gaussian distribution을 정의하면,<br />
지수항을 더하여 새로운 Gaussian form으로 강제로 만들어 주면 다음의 Posterioral의 평균과 분산을 구할 수 있다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-024.png" /><br />
즉, 쉽게 말해서 Prior과 likelihood의 결합으로 posterioral이 형성이 되는데,<br />
그 posterioral에 영향을 주는 정도가 분산으로 나타난다.<br />
(b)는 prior의 분산이 커 posterial에 주는 영향이 작음을 알 수 있다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-025.png" /></p>
<h1>Gaussian - Gaussian Model (Multivariant)</h1>
<p>(우리는 ch3에서 전부 다룬 여러 pdf를 하나씩 prior likelihood, posterioral을 작성해보는 중이다)<br />
이번에도 ch3에서 배운 다변수 가우시안 분포를 살펴보자.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-026.png" /><br />
Likelihood of MGM<br />
Prior has to be in the same Family, Multivariant Gaussian Distribution<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-027.png" /><br />
Prior distribution<br />
ch3에서 수학적으로 다룬 내용이므로, 결과만 작성하면,<br />
Posterioral 의 평균과 분산은 다음과 같다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-028.png" /><br />
밑의 예시가 정확히 우리가 무엇을 하고있는 지를 보여준다.<br />
Prior은 정직한 Multivariant Gaussian distribution이라고 설정하고,<br />
Data를 우리가 가지고 있을때, posterial(prior based model)을 추론 할 수 있다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-029.png" /></p>
<h1>Hierarichal priors</h1>
<p>posterial을 구하기 위해서, p(theat) - prior을 정의해야한다.<br />
이때 prior정의를 위해 필요한 파라메터를 새롭게 정의할 수 있다.<br />
ξ: hyperparameter<br />
따라서, 다음의 계급적 순서가 생기게 된다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-030.png" /><br />
그리고, 확률분포는 다음과 같이 bayes rule사용.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-031.png" /><br />
이를 MAP문제에 적용하면, -&gt; 최적의 hyperparameter를 찾는 문제로 전환할 수 가 있다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-032.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-033.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-032.png" /><br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-033.png" /><br />
여기서 재미있는 사실은, 최적의 hyperparameter를 찾기 위해서 Data를 사용한다는 사실이다.<br />
즉, prior의 정의자체가 data가 이전의 순수한 믿음인데,<br />
Data를 이용하여 정의한다는 점이 역설적이다.<br />
(실제로 정확한 posterial을 얻기 위해서, Data를 미리 보는 컨닝을 하는 느낌)<br />
(called Empirical Bayes(EB))<br />
따라서, 우리는 loglikelihood를 최적화하는 변수와 방법이 여러가지가 있다는 것을 지금까지 확인하였다.<br />
<img alt="Ch4 Statistics - Bayesian Statistics" src="./images/img-034.png" /></p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>