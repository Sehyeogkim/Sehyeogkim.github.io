<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ch4. Gradient decent algorithm | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
</head>
<body>
  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Knowledge Base</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/advanced-engineering-mathematics/" class="nav-item">Advanced_Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item active">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/machine-learning/" class="nav-item">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/numerical-heat-transfer-and-fluid-flow/" class="nav-item">Numerical-Heat-transfer-and-Fluid-flow<span class="nav-post-count">14</span></a>
        <a href="/blog/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/deep-learning/">Deep-learning</a><span class="sep">/</span>  <span>Ch4. Gradient decent algorithm</span></div>
<a href="/blog/deep-learning/" class="back-link">&larr; Back to Deep-learning</a>
<div class="page-header"><h1>Ch4. Gradient decent algorithm</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2024-09-25</span><span class="meta-item"><span class="meta-label">Category:</span> Deep-learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/99" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>오늘은, 그레디언트 디센트 알고리즘,<br />
즉 연산 Itereation마다<br />
w값을 수정할 텐데<br />
어떤 방식으로 수정하는지를 살펴보자.<br />
먼저,<br />
Batch Gradient Descent<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-001.png" /><br />
이전 포스터에서 구한, loss funcition<br />
(log logistic likelihood function)<br />
의 w에 대한 gradient를 모든 y값들에 대해서 더하고,<br />
y값의 갯수로 나누어주어,<br />
평균값을 gradient 로 정의한다.<br />
그리고, step size = alpha를 곱해주어<br />
얼마나 w를 변화시킬 지를 계산한다.<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-002.png" /><br />
위와 같은 방법으로, 점점 최소한의 cost<br />
로 도달하는 w로 변화한다.<br />
Stochastic Gradient Descent<br />
batch gradient decent 는 straightforward하지만,<br />
밑의 사진의 왼쪽 하늘색 공처럼<br />
local minimum에 갇힐 확률이 굉장히 높다.<br />
이를 위해서, Stochastic Gradient decent는<br />
매순간, 랜덤한 데이터들을 고르고,<br />
그에 대한 w, cost gradient를 계산한다.<br />
때로는, gatch gradient decent보다 연산량이 많을 수 있지만,<br />
적어도 Local minimum에 빠지는 일은 적다.<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-003.webp" /><br />
마지막으로는, 연산시간을 굉장히 줄이는 방법중 하나인<br />
Mini-batch Gradient decent 이다.<br />
train data 전부를 가지고, w를 수정하는 것이 아니라,<br />
랜덤하게 하나의 batch (Group)을 데이터에서 추출한다.<br />
랜덤하기 때문에, 골고루 펴져있는 데이터들일 것임,<br />
따라서, 이들을 가지고 w를 계속해서 업데이트 한다면<br />
밑의 사진처럼 minium에 빠르게 도달 할 수 있게된다.<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-004.png" /><br />
추가로, Gradient decent method들의<br />
단점에 대해서 언급하고,<br />
마무리 하도록 하겠습니다.<br />
경사하강법의 가장 큰 단점은<br />
Learning rate 설정입니다.<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-005.png" /><br />
적절한 Learning rate: alpha를 설정해야지,<br />
최적의 연산속도로 최적의 해답을 찾을 수 있습니다.<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-006.png" /><br />
따라서, 최적의 learning rate는<br />
어떻게 정의할까?<br />
매순간 스텝사이즈를 다르게 정의해주는데,<br />
밑의 두가지를 고려하여 정의해줍니다.<br />
1. 기울기 크기에 따라 학습률을 조정 :<br />
기울기가 크면 더 작은 스텝을, 기울기가 작으면 더 큰 스텝<br />
2.이전 반복에서의 성과에 따라 학습률 조정 :<br />
이전 스텝이 잘 작동했으면 학습률을 늘리고, 잘 작동하지 않았으면 줄임<br />
위 두가지 사항을 전부 식에 고려해준 방법이 바로.<br />
Adam (Adaptive Moment Estimation)<br />
<img alt="Ch4. Gradient decent algorithm" src="./images/img-007.png" /><br />
위 식을 보면,<br />
가운데 항이<br />
1. Loss function의 Gradient를 고려<br />
마지막 항이<br />
2. 이전 iteration의 기울기를 고려하여<br />
이전 iteartion 기울기가 크다면, 지금 굉장히 w가 많이 변화 했을 것이다.<br />
이 momentum을 그대로 이어가, w를 수정하면<br />
굉장히 빠른 시간에 수정할 수 있다.<br />
(α<br />
는 모멘텀 계수로 = 0.9로 보통 설정)</p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>