<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Recurrent Neural Network(RNN) &amp; LSTM | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
</head>
<body>
  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Knowledge Base</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/advanced-engineering-mathematics/" class="nav-item">Advanced_Engineering_Mathematics<span class="nav-post-count">14</span></a>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular_Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item active">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid_Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas_Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/machine-learning/" class="nav-item">Machine_Learning<span class="nav-post-count">11</span></a>
        <a href="/blog/numerical-heat-transfer-and-fluid-flow/" class="nav-item">Numerical-Heat-transfer-and-Fluid-flow<span class="nav-post-count">14</span></a>
        <a href="/blog/sensitivity-analysis/" class="nav-item">Sensitivity_Analysis<span class="nav-post-count">3</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid_Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous_Flow<span class="nav-post-count">28</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/deep-learning/">Deep-learning</a><span class="sep">/</span>  <span>Recurrent Neural Network(RNN) &amp; LSTM</span></div>
<a href="/blog/deep-learning/" class="back-link">&larr; Back to Deep-learning</a>
<div class="page-header"><h1>Recurrent Neural Network(RNN) &amp; LSTM</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2024-12-11</span><span class="meta-item"><span class="meta-label">Category:</span> Deep-learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/m/144" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>Recurrent Neural Network(RNN) &amp; LSTM<br />
지금까지 많은<br />
Neueral Network<br />
구조 들에 대해서<br />
배워왔다.<br />
모든 network들은 살펴보면,<br />
각각의 특정 목적에 맞게 설계되었다.<br />
이번에는<br />
time - series data<br />
를 가지고<br />
다음 time을 예측하는 문제를 풀어보자.<br />
(Recurrent Neural Network)<br />
결론적으로, RNN구조는 다음과 같다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-001.png" /><br />
오른쪽으로 갈수록 시간이 흐르고,<br />
Y가 시간에 따른 input 데이터 라고 하자<br />
이때, 특정 layer을 지나, X - latent state value<br />
를 각각 시간마다 구할 수 있다.<br />
그리고, 우리가 하고 싶은 것은<br />
시간에 따른 흐름<br />
이다.<br />
결국, 시간의 흐름에 따라 Latent value가 어떻게 바뀌는 지<br />
'Dynamic'<br />
을 가지고,<br />
최종 output 을 도출한다<br />
여기서 조금 더 복잡하게 RNN구조를 그려보면,<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-002.png" /><br />
gray: input, green: latent, Orange: Output<br />
핵심은 latent values들이 전부 연결되어 있다는 것이다.<br />
따라서, 시간 t = n 일때 이전 t =0 -&gt; (n-1)의 모든<br />
latent 정보들은 t = n , latent에 Input으로 들어간다.<br />
여기서 시간이 t일때,<br />
ht는 ht-1과 xt의 조합으로<br />
이루어질 것이다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-003.png" /><br />
이를 함수로 나타내면 다음과 같다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-004.png" /></p>
<hr />
<p>[Long-short term memory(LSTM)]<br />
여기서 재밌는 것은,<br />
이전 시간의 모든 정보들을 다 담는 것은 굉장히 비효율적이다.<br />
gradient를 계산할때 굉장히 오래거리고,<br />
gradient vanishing 문제도 발생한다.<br />
따라서,<br />
과거정보들중<br />
필요한 정보<br />
들만 선택적으로 저장할수는 없을까?<br />
예를들어, 시간 t+1일때, output h_t+1을 구하고 싶은데,<br />
이때 중요한 과거 데이터는 x_0, x_1밖에 없다.<br />
즉, 나머지 x_2 ... x_t는 무시할 수 없을까???<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-005.png" /><br />
RNN example<br />
해결책은 연산 과정속에서,<br />
이렇게 gate를 설치하는 것이다.<br />
원하는 흐름으로 계산하도록 유도.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-006.png" /><br />
이 게이트 열고 닫음을<br />
구동시키는 매커니즘이 바로<br />
LSTM이다.<br />
LSTM에서 Gate는 다음과 같은 연산방식으로,<br />
흘러 나가는 정보의 양을 조절한다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-007.png" /><br />
구체적으로,<br />
LSTM이 정확히 어떤 구조의 flow로<br />
선택적으로 정보를 저장하는 살펴보자.<br />
기본적인 RNN time: t일때,<br />
연산과정에서부터 시작해보자.<br />
밑의 사진처럼,<br />
2개의 정보<br />
이전시간의 latent 정보[ h_(t-1) ] ,<br />
현재 input data [x_t] 를 가지고,<br />
현재 시간의 output [h_t]를 predict하는 구조이다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-008.png" /><br />
여기서<br />
Forgetting gate<br />
를 추가해보자.<br />
바로, x_t, h_t-1중에서<br />
선택적으로 어느 정보를 지울지를 학습한다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-009.png" /><br />
여기에 더하여, 최종적으로 prediction으로 선택적으로 보내기 위한<br />
selection gate<br />
도 설치한다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-010.png" /><br />
forgeting 이전에 아예 정보를 무시하기 위한<br />
ignoring gate<br />
도 설치해준다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-011.png" /><br />
위 diagram을 LSTM CELL로 표시한 사진은 다음과 같다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-012.jpg" /><br />
정리하자면, 총 3가지 gate가 존재한다.<br />
ht-1, xt 정보를 가지고<br />
1. 이전 Cell 정보에서 어디까지 잊을지 정하고<br />
(forgetting gate)<br />
2. ht-1, xt 정보를 cell 정보에 어디까지 더할지 정하고<br />
(Ignoring gate)<br />
3. 최종 output(h_t)를 어디까지 정할지.<br />
(Selection gate)<br />
지금까지는 LSTM 특정 시간(t)에 대한<br />
1개 cell의 구조에 대해서<br />
알아 보았고,<br />
실제로는 이제 여러시간에 따른 series구조이기 때문에<br />
다음과 같이, LSTM cell여러개가 in sequence한 형태이다.<br />
<img alt="Recurrent Neural Network(RNN) &amp; LSTM" src="./images/img-013.png" /></p>
<h1>LSTM, RNN의 문제점</h1>
<p>일단, 계산량이 너무 많다.<br />
선택적으로 정보를 저장하더라도, 저장해야할<br />
정보의 양도 너무 많다.</p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>