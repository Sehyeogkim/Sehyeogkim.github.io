<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ch3_2. Classification_Logistic Regression | Sehyeog Kim</title>
  <link rel="stylesheet" href="../../../assets/css/style.css">
</head>
<body>
  <!-- Mobile header -->
  <header class="mobile-header">
    <span class="site-title">Sehyeog Kim</span>
    <button class="menu-toggle" aria-label="Menu">&#9776;</button>
  </header>
  <div class="sidebar-overlay"></div>

  <div class="site-wrapper">
    <!-- Sidebar -->
    <aside class="sidebar">
      <div class="sidebar-bg">
        <img src="../../../assets/images/bg.jpg" alt="Background"
             onerror="this.style.display='none'">
      </div>
      <div class="sidebar-profile">
        <img class="profile-photo"
             src="../../../assets/images/profile.jpg"
             alt="Sehyeog Kim"
             onerror="this.style.background='#21262d'">
        <h1 class="profile-name">Sehyeog Kim</h1>
        <p class="profile-bio">AI &amp; Computational Engineering<br>Knowledge Base</p>
        <div class="profile-links">
          <a href="https://github.com/Sehyeogkim" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg> GitHub
          </a>
        </div>
      </div>
      <nav class="sidebar-nav">
        <a href="/" class="nav-item nav-home">Home</a>
        <span class="nav-label">Categories</span>
        <a href="/blog/agentic-ai/" class="nav-item">Agentic_AI<span class="nav-post-count">8</span></a>
        <a href="/blog/blood-flow-and-metabolism/" class="nav-item">Blood-Flow-and-Metabolism<span class="nav-post-count">12</span></a>
        <a href="/blog/cardiovascular-diseases/" class="nav-item">CardioVascular-Diseases<span class="nav-post-count">8</span></a>
        <a href="/blog/computational-linear-algebra/" class="nav-item">Computational-Linear-Algebra<span class="nav-post-count">15</span></a>
        <a href="/blog/continuum-mechanics/" class="nav-item">Continuum-Mechanics<span class="nav-post-count">9</span></a>
        <a href="/blog/deep-learning/" class="nav-item active">Deep-learning<span class="nav-post-count">14</span></a>
        <a href="/blog/finite-element-method/" class="nav-item">Finite-Element-Method<span class="nav-post-count">1</span></a>
        <a href="/blog/fluid-mechanics/" class="nav-item">Fluid-Mechanics<span class="nav-post-count">18</span></a>
        <a href="/blog/gas-dynamics/" class="nav-item">Gas-Dynamics<span class="nav-post-count">24</span></a>
        <a href="/blog/heat-transfer/" class="nav-item">Heat-transfer<span class="nav-post-count">8</span></a>
        <a href="/blog/math/" class="nav-item">math<span class="nav-post-count">0</span></a>
        <a href="/blog/numerical-heat-transfer-and-fluid-flow/" class="nav-item">Numerical-Heat-transfer-and-Fluid-flow<span class="nav-post-count">14</span></a>
        <a href="/blog/solid-mechanics/" class="nav-item">Solid-Mechanics<span class="nav-post-count">25</span></a>
        <a href="/blog/thermodynamics/" class="nav-item">Thermodynamics<span class="nav-post-count">14</span></a>
        <a href="/blog/viscous-flow/" class="nav-item">Viscous-Flow<span class="nav-post-count">28</span></a>
        <a href="/blog/과학/" class="nav-item">과학<span class="nav-post-count">26</span></a>
        <a href="/blog/취미/" class="nav-item">취미<span class="nav-post-count">2</span></a>
      </nav>
    </aside>

    <!-- Main content -->
    <main class="main-content">
      <div class="breadcrumb">  <a href="/">Home</a><span class="sep">/</span>  <a href="/blog/deep-learning/">Deep-learning</a><span class="sep">/</span>  <span>Ch3_2. Classification_Logistic Regression</span></div>
<a href="/blog/deep-learning/" class="back-link">&larr; Back to Deep-learning</a>
<div class="page-header"><h1>Ch3_2. Classification_Logistic Regression</h1></div>
<div class="post-meta"><span class="meta-item"><span class="meta-label">Date:</span> 2024-09-25</span><span class="meta-item"><span class="meta-label">Category:</span> Deep-learning</span><span class="meta-item"><span class="meta-label">Source:</span> <a href="https://jeffdissel.tistory.com/98" target="_blank" rel="noopener">link</a></span></div>
<article class="post-content"><p>지난 포스터에서는,<br />
class가 다른 점들을 분류하는<br />
구분선을 어떻게 정의하는지<br />
에 대해서 알아보았다.<br />
그렇다면, 이런 질문이 들 수 있다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-001.png" /><br />
위 두 직선 모두, 서로 다른 클라쓰의 두점을<br />
구분하였다고 가정하자.<br />
그랬을때, 두 구분선중에서<br />
무엇이 더 잘 구분한 구분선인가??<br />
바로, 점들과 구분선 사이의 거리의 합이 최소일때, 즉<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-002.png" /><br />
산술기하평균에서 배웠듯이,<br />
거리의 곱들이 최소인 순간이다.<br />
거리의 표현이<br />
g(x) / ||w||<br />
라는 것은 지난시간 증명 완료 하였고,<br />
결국, w,x의 내적과 비례하다는 것을 도출 할 수 있다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-003.png" /><br />
이후, 거리의 범위가 너무 넓기 때문에<br />
강제로, 0과 1사이의 범위로 축소해준다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-004.png" /><br />
범위가 무한인 h를 0-1사이의 범위로<br />
스케일 다운 시켜주는 함수를<br />
logistic function<br />
이라고 부르고 다양한 종류가 존재한다.<br />
그 중에서, 대표적인 함수는 sigmoid function<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-005.png" /><br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-006.png" /><br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-005.png" /><br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-006.png" /><br />
sigmoid function graph<br />
그렇다면, 왜 0-1사이로 바꾸었는가?/<br />
바로 확률의 관점으로 계산 할 수 있기 때문이다!.<br />
도출 값이 1일 확률, 과 0 일 확률 두가지를<br />
다음과 같이 시그모이드 함수로 나타낼 수 있다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-007.png" /><br />
다시, logistic function hw(x) 형태로 나타내고,<br />
그중에서 우리는 시그모이드 함수를 사용할 것이다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-008.png" /><br />
위의 식을 합쳐서 표현해보면.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-009.png" /><br />
위 확률의 의미를 정확히 이해하고 넘어가자.<br />
데이터상 x = 0 y = 1이라고 하자.<br />
그렇다면, 지금 현재 w는<br />
P(y=1 | x;w) = hw(x) 를 증가 시켜야 한다.<br />
(y=1일 확률)<br />
만약에, x=0, y=0 인 경우는<br />
현재 w는<br />
1-hw(x) 를 증가시켜야 한다.<br />
(y=1일 확률을 낮춰야함)<br />
따라서, 결론적으로<br />
P(y | x;w)가 의미하는 것은, w(변수)가 올게 판단하는 정도를<br />
나타낸다.<br />
자 이제 w 기준으로, 각각 모든 데이터의 확률을 곱한 것이 바로.<br />
L(w): Likelihodd function 이고<br />
이는, 지금 w 기준으로,<br />
모든 데이터들에 대하여 올바르게 판단한 정도를 나타낸다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-010.png" /><br />
따라서, L(w) 를 최대화 시키는 w를 찾아야 한다.<br />
지수형태이므로, log를 씌우면 더 간단하게 다음과같이 나타낼 수 있고,<br />
log likelihood function<br />
이라고 부른다.<br />
<img alt="Ch3_2. Classification_Logistic Regression" src="./images/img-011.png" /><br />
즉, l(w) 를 최대로 하는<br />
-l(w)를 최소로하는<br />
optimization 문제라는 것이다.<br />
따라서, deep learning이 최적화 라는 것.</p></article>
      <footer class="site-footer">
        <p>&copy; 2026 Sehyeog Kim. Built with gitfolio-inspired theme.</p>
      </footer>
    </main>
  </div>

  <script src="../../../assets/js/main.js"></script>
</body>
</html>