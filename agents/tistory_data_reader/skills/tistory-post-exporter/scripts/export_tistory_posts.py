#!/usr/bin/env python3
"""Export Tistory posts into per-post folders with content.txt and images/."""

from __future__ import annotations

import argparse
import json
import re
import sys
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup, NavigableString, Tag


USER_AGENT = "Mozilla/5.0 (compatible; tistory-post-exporter/1.0)"
TIMEOUT = 30


@dataclass
class ExportResult:
    url: str
    status: str
    post_id: str | None = None
    out_dir: str | None = None
    image_count: int = 0
    error: str | None = None


def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": USER_AGENT})
    return s


def normalize_blog_url(url: str) -> str:
    parsed = urlparse(url)
    if not parsed.scheme:
        url = "https://" + url
        parsed = urlparse(url)
    if not parsed.netloc:
        raise ValueError(f"Invalid blog URL: {url}")
    return f"{parsed.scheme}://{parsed.netloc}"


def discover_post_urls(session: requests.Session, blog_url: str) -> list[str]:
    sitemap_url = f"{blog_url}/sitemap.xml"
    resp = session.get(sitemap_url, timeout=TIMEOUT)
    resp.raise_for_status()
    xml = BeautifulSoup(resp.text, "xml")
    urls = []
    for loc in xml.select("url > loc"):
        u = (loc.get_text() or "").strip()
        if re.search(r"/\d+$", u):
            urls.append(u)
    unique = sorted(set(urls), key=lambda x: int(re.search(r"/(\d+)$", x).group(1)))
    return unique


def post_id_from_url(url: str) -> str | None:
    m = re.search(r"/(\d+)$", url)
    return m.group(1) if m else None


def slugify(text: str) -> str:
    text = text.lower().strip()
    text = re.sub(r"[^a-z0-9]+", "-", text)
    text = re.sub(r"-{2,}", "-", text).strip("-")
    return text or "post"


# Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
def safe_segment(text: str, fallback: str) -> str:
    normalized = unicodedata.normalize("NFKC", (text or "").strip())
    normalized = normalized.replace("/", "-").replace("\\", "-")
    normalized = re.sub(r"\s+", "-", normalized)
    normalized = re.sub(r"[^\w\-]+", "", normalized, flags=re.UNICODE)
    normalized = re.sub(r"-{2,}", "-", normalized).strip("-")
    return normalized or fallback


def clean_text(text: str) -> str:
    text = text.replace("\xa0", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text


def image_ext(url: str) -> str:
    path = urlparse(url).path.lower()
    if path.endswith(".png"):
        return ".png"
    if path.endswith(".jpg") or path.endswith(".jpeg"):
        return ".jpg"
    if path.endswith(".webp"):
        return ".webp"
    if path.endswith(".gif"):
        return ".gif"
    return ".jpg"


def parse_blocks(body: Tag, post_url: str) -> list[tuple[str, str]]:
    blocks: list[tuple[str, str]] = []
    skip_classes = {"another_category", "related_posts", "tt-box-text"}

    for node in body.descendants:
        if isinstance(node, NavigableString):
            continue
        if not isinstance(node, Tag):
            continue
        if node.name in {"script", "style", "noscript", "button", "input", "textarea"}:
            continue
        if set(node.get("class", [])) & skip_classes:
            continue

        if node.name in {"h1", "h2", "h3", "h4", "h5", "h6"}:
            t = clean_text(node.get_text(" ", strip=True))
            if t:
                blocks.append(("text", t))
            continue

        if node.name in {"p", "li", "blockquote"}:
            emitted = False
            for part in node.contents:
                if isinstance(part, NavigableString):
                    t = clean_text(str(part))
                    if t:
                        blocks.append(("text", t))
                        emitted = True
                    continue
                if not isinstance(part, Tag):
                    continue
                if part.name == "img":
                    src = part.get("src") or part.get("data-src")
                    if src:
                        blocks.append(("image", urljoin(post_url, src)))
                        emitted = True
                    continue
                nested_imgs = part.select("img")
                if nested_imgs:
                    for im in nested_imgs:
                        src = im.get("src") or im.get("data-src")
                        if src:
                            blocks.append(("image", urljoin(post_url, src)))
                            emitted = True
                else:
                    t = clean_text(part.get_text(" ", strip=True))
                    if t:
                        blocks.append(("text", t))
                        emitted = True
            if not emitted:
                t = clean_text(node.get_text(" ", strip=True))
                if t:
                    blocks.append(("text", t))
            continue

        if node.name == "figure":
            imgs = node.select("img")
            for im in imgs:
                src = im.get("src") or im.get("data-src")
                if src:
                    blocks.append(("image", urljoin(post_url, src)))
            cap = node.select_one("figcaption, .cap1, .imageblock-caption")
            if cap:
                t = clean_text(cap.get_text(" ", strip=True))
                if t:
                    blocks.append(("text", t))
            continue

    dedup: list[tuple[str, str]] = []
    for item in blocks:
        if dedup and dedup[-1] == item:
            continue
        dedup.append(item)
    return dedup


# Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
def detect_category(soup: BeautifulSoup) -> str:
    section = soup.select_one('meta[property="article:section"]')
    if section and section.get("content"):
        return clean_text(section["content"])

    candidates = soup.select(
        "a[href*='/category/'], .category a, .post-category a, .tt_category a"
    )
    for item in candidates:
        text = clean_text(item.get_text(" ", strip=True))
        if text:
            return text

    return "uncategorized"


def extract_post(
    session: requests.Session, post_url: str
) -> tuple[str, str, str, str, list[tuple[str, str]]]:
    resp = session.get(post_url, timeout=TIMEOUT)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    article = soup.select_one("article")
    body = soup.select_one(".tt_article_useless_p_margin") or article
    if body is None:
        raise RuntimeError("Cannot find article body.")

    title = ""
    og_title = soup.select_one('meta[property="og:title"]')
    if og_title and og_title.get("content"):
        title = clean_text(og_title["content"])
    if not title:
        h1 = soup.select_one("article h1") or soup.select_one("h1")
        title = clean_text(h1.get_text(" ", strip=True)) if h1 else "untitled-post"

    date = ""
    pub = soup.select_one('meta[property="article:published_time"]')
    if pub and pub.get("content"):
        date = clean_text(pub["content"])
    if not date:
        info = soup.select_one(".article_info")
        if info:
            date = clean_text(info.get_text(" ", strip=True))

    # Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
    category = detect_category(soup)
    blocks = parse_blocks(body, post_url)
    return title, post_url, date, category, blocks


# Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
def find_existing_post_dir_by_id(out_root: Path, post_id: str) -> Path | None:
    # Old code (commented): Resume check only looked at a single calculated folder path.
    # New code checks all existing content.txt files and matches by PostID header.
    for content_path in out_root.rglob("content.txt"):
        try:
            raw = content_path.read_text(encoding="utf-8", errors="ignore")
        except Exception:  # noqa: BLE001
            continue
        for line in raw.splitlines()[:12]:
            if line.strip() == f"PostID: {post_id}":
                return content_path.parent
    return None


def export_one(
    session: requests.Session,
    post_url: str,
    out_root: Path,
    overwrite: bool,
    resume: bool,
) -> ExportResult:
    pid = post_id_from_url(post_url)
    if not pid:
        return ExportResult(url=post_url, status="failed", error="Post URL must end with numeric ID.")

    try:
        # Old code (commented): title, url, date, blocks = extract_post(session, post_url)
        # Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
        title, url, date, category, blocks = extract_post(session, post_url)
    except Exception as e:  # noqa: BLE001
        return ExportResult(url=post_url, status="failed", post_id=pid, error=str(e))

    # Old code (commented): slug = slugify(title)
    # Old code (commented): post_dir = out_root / f"post-{pid}-{slug}"
    # Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
    category_dirname = safe_segment(category, "uncategorized")
    title_dirname = safe_segment(title, f"post-{pid}")
    post_dir = out_root / category_dirname / title_dirname
    content_path = post_dir / "content.txt"
    img_dir = post_dir / "images"

    # Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
    existing_dir_by_id = find_existing_post_dir_by_id(out_root, pid)

    # Old code (commented):
    # if content_path.exists() and resume and not overwrite:
    #     return ExportResult(...)
    if resume and not overwrite and existing_dir_by_id is not None:
        existing_img_dir = existing_dir_by_id / "images"
        return ExportResult(
            url=post_url,
            status="skipped",
            post_id=pid,
            out_dir=str(existing_dir_by_id),
            image_count=len(list(existing_img_dir.glob("*"))) if existing_img_dir.exists() else 0,
        )

    if content_path.exists() and resume and not overwrite:
        return ExportResult(
            url=post_url,
            status="skipped",
            post_id=pid,
            out_dir=str(post_dir),
            image_count=len(list(img_dir.glob("*"))) if img_dir.exists() else 0,
        )

    post_dir.mkdir(parents=True, exist_ok=True)
    img_dir.mkdir(parents=True, exist_ok=True)

    img_map: dict[str, str] = {}
    image_index = 0
    lines = [
        f"Title: {title}",
        f"URL: {url}",
        f"PostID: {pid}",
        f"Date: {date}",
        # Generated by ChatGPT (Codex) at 2026-02-25 21:45:27 KST
        f"Category: {category}",
        "",
    ]

    for kind, val in blocks:
        if kind == "text":
            lines.append(val)
            continue

        src = val
        if src not in img_map:
            image_index += 1
            filename = f"img-{image_index:03d}{image_ext(src)}"
            out_img = img_dir / filename
            try:
                r = session.get(src, timeout=TIMEOUT)
                r.raise_for_status()
                out_img.write_bytes(r.content)
            except Exception:  # noqa: BLE001
                # Keep deterministic naming even on failed download.
                out_img.write_bytes(b"")
            img_map[src] = filename
        lines.append(f"[IMAGE: images/{img_map[src]}]")

    content_path.write_text("\n".join(lines).strip() + "\n", encoding="utf-8")
    return ExportResult(
        url=post_url,
        status="ok",
        post_id=pid,
        out_dir=str(post_dir),
        image_count=len(img_map),
    )


def parse_args(argv: Iterable[str]) -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Export Tistory posts into per-post directories with content.txt and images/."
    )
    p.add_argument("--post-url", action="append", default=[], help="Single post URL. Repeatable.")
    p.add_argument("--blog-url", help="Blog root URL (e.g. https://jeffdissel.tistory.com).")
    p.add_argument("--all-posts", action="store_true", help="Discover all posts via sitemap.xml.")
    p.add_argument("--out-root", required=True, help="Output root directory.")
    p.add_argument("--limit", type=int, default=0, help="Process only first N discovered posts.")
    p.add_argument("--resume", action="store_true", help="Skip already-exported posts.")
    p.add_argument("--overwrite", action="store_true", help="Overwrite existing post outputs.")
    p.add_argument(
        "--report-file",
        default="",
        help="Optional JSON report file path. If omitted, prints summary only.",
    )
    return p.parse_args(list(argv))


def main(argv: Iterable[str]) -> int:
    args = parse_args(argv)
    out_root = Path(args.out_root).expanduser().resolve()
    out_root.mkdir(parents=True, exist_ok=True)

    session = make_session()
    targets = list(args.post_url)

    if args.all_posts:
        if not args.blog_url:
            print("ERROR: --blog-url is required with --all-posts", file=sys.stderr)
            return 2
        blog_url = normalize_blog_url(args.blog_url)
        try:
            discovered = discover_post_urls(session, blog_url)
        except Exception as e:  # noqa: BLE001
            print(f"ERROR: Failed to discover post URLs: {e}", file=sys.stderr)
            return 2
        if args.limit > 0:
            discovered = discovered[: args.limit]
        targets.extend(discovered)

    # keep order, remove duplicates
    seen = set()
    uniq_targets = []
    for t in targets:
        if t in seen:
            continue
        seen.add(t)
        uniq_targets.append(t)

    if not uniq_targets:
        print("ERROR: No targets. Provide --post-url or --all-posts with --blog-url.", file=sys.stderr)
        return 2

    results: list[ExportResult] = []
    for url in uniq_targets:
        result = export_one(session, url, out_root=out_root, overwrite=args.overwrite, resume=args.resume)
        results.append(result)
        label = result.status.upper()
        print(f"[{label}] {url}")
        if result.out_dir:
            print(f"  -> {result.out_dir}")
        if result.error:
            print(f"  !! {result.error}")

    summary = {
        "processed": sum(1 for r in results if r.status == "ok"),
        "skipped": sum(1 for r in results if r.status == "skipped"),
        "failed": sum(1 for r in results if r.status == "failed"),
        "results": [r.__dict__ for r in results],
    }

    print("\nSummary:")
    print(json.dumps(summary, ensure_ascii=False, indent=2))

    if args.report_file:
        report_path = Path(args.report_file).expanduser().resolve()
        report_path.parent.mkdir(parents=True, exist_ok=True)
        report_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")

    return 0 if summary["failed"] == 0 else 1


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
