<!--
# Project Objective

Extract all posts from `https://jeffdissel.tistory.com/` for static publishing (GitHub Pages ready format).

# Required Output Structure

- Create one directory per post.
- Directory naming:
  - Use `post-{id}-{slug}` when possible.
  - If slug is unclear, use `post-{id}`.
- Inside each post directory, create:
  - `content.txt`: full post text in original order.
  - `images/`: all images used in that post.

# content.txt Rules

- Keep text blocks in the same order as the original post.
- For each image position in the article, insert a marker line at the exact location:
  - `[IMAGE: images/<saved-file-name>]`
- Do not move all image references to the end; keep them inline where they appeared.
- Include minimal metadata header at the top:
  - `Title: ...`
  - `URL: ...`
  - `PostID: ...`
  - `Date: ...` (if available)

# Image Rules

- Download original image sources when accessible.
- Save into each post's local `images/` directory only.
- Use deterministic names in appearance order:
  - `img-001.<ext>`, `img-002.<ext>`, ...
- Match `content.txt` marker paths exactly.

# Execution Behavior

- Do not start bulk extraction until user explicitly says to start.
- Before running extraction, confirm output root directory once.
- Make the process resumable:
  - Skip already completed posts unless user asks to re-run.
  - Avoid duplicate image downloads when files already exist.
- Never store credentials in files or commit history.

# Quality Checks (per run)

- Verify each processed post has:
  - `content.txt`
  - at least zero or more image files in `images/`
  - all `[IMAGE: ...]` markers pointing to existing files
- Report summary:
  - number of posts processed
  - number skipped
  - number failed (with URLs)
-->

# Tistory Data Reader - Subagents Definition

Generated by ChatGPT (Codex) at: 2026-02-25 21:32:11 KST

## Goal

Extract all posts from `https://jeffdissel.tistory.com/` into static-friendly post folders for GitHub Pages workflows.

## Output Contract

- Root: `/Users/jeff/blog/Sehyeogkim.github.io/blog_data/Tistory`
- Per post:
  - `post-{id}-{slug}/content.txt`
  - `post-{id}-{slug}/images/img-001.<ext>`, `img-002.<ext>`, ...
- `content.txt` metadata header:
  - `Title: ...`
  - `URL: ...`
  - `PostID: ...`
  - `Date: ...`
- Inline image markers at exact positions:
  - `[IMAGE: images/<saved-file-name>]`

## Subagents

### 1) indexer

- Responsibility:
  - Discover all candidate post URLs from sitemap (`/sitemap.xml`) and optional user-provided seed URLs.
  - Build deterministic processing order by PostID ascending.
- Input:
  - `blog_url`
  - optional `seed_post_urls[]`
- Output:
  - `post_targets[]` with fields:
    - `url`
    - `post_id`
    - `discovered_at`

### 2) deduper

- Responsibility:
  - Decide skip/process/retry using resumable rules.
  - Prevent duplicate processing when titles overlap.
- Primary key policy:
  - Always use `PostID` first, never title for identity.
- Skip policy:
  - Skip when matching `PostID` output exists and `content.txt` exists.
  - Skip image download for existing files during resume.
  - If slug changed but same `PostID`, treat as same post.
- Input:
  - `post_targets[]`
  - `out_root`
- Output:
  - `to_process[]`
  - `to_skip[]`
  - `skip_reason`

### 3) exporter

- Responsibility:
  - Read each target post.
  - Extract ordered text blocks and image URLs.
  - Save `content.txt` and `images/` with deterministic naming.
- Runtime policy:
  - Default to resume mode for long-running jobs.
  - Do not overwrite unless explicitly requested.
  - Keep image marker order exactly where images appeared.
- Input:
  - `to_process[]`
  - `out_root`
  - mode flags (`resume`, `overwrite`, `limit`)
- Output:
  - Per-post result:
    - `status` (`ok` | `skipped` | `failed`)
    - `url`
    - `post_id`
    - `out_dir`
    - `image_count`
    - `error` (if any)

### 4) validator

- Responsibility:
  - Verify output integrity after each run.
- Checks:
  - `content.txt` exists.
  - `images/` directory exists.
  - Every `[IMAGE: images/<file>]` marker points to an existing file.
- Output:
  - Run summary:
    - processed count
    - skipped count
    - failed count
    - failed URL list with reasons

## Execution Order

1. `indexer` collects and sorts all candidate posts by PostID ascending.
2. `deduper` resolves skip/process sets via PostID-based resume rules.
3. `exporter` processes `to_process[]` in fixed order.
4. `validator` performs integrity checks and prints final summary.

## Safety Rules

- Do not store credentials in files or git history.
- Do not start bulk run unless user explicitly asks to start.
- Confirm output root once before first bulk execution.

## Recommended Run Pattern

- Pilot:
  - small batch with `--limit` for validation
- Full run:
  - `--resume` enabled by default
- Re-extraction:
  - use overwrite only on explicit user request
